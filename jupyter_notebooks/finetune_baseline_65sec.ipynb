{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune: Baseline (65 second)\n",
        "\n",
        "In this notebook, we perform full finetuning on the PhysioNet dataset using a randomly initialized ResNet18. This will form the baseline to which we will compare the performance of the pre-trained ResNet18 on the Icentia11K dataset.\n",
        "\n",
        "## Computational Requirements\n",
        "\n",
        "We run this notebook in Google Colab Pro to utilize GPU resources. We perform finetuning using the **V100 GPU**.\n",
        "\n",
        "## Data\n",
        "\n",
        "We have prepared the PhysioNet data in a separate notebook ([Github](https://github.com/myles-i/DLH_TransferLearning/blob/master/jupyter_notebooks/finetuning_explore.ipynb)). That notebook generates _train_ and _test_ datasets.\n",
        "\n",
        "In our case, the datasets are saved to our Google Drive at these paths:\n",
        "\n",
        "1. `/content/drive/MyDrive/Project/data/physionet_finetune/physionet_train.pkl`\n",
        "2. `/content/drive/MyDrive/Project/data/physionet_finetune/physionet_test.pkl`\n",
        "\n",
        "On this data, the preprocessing is as follows:\n",
        "1. Downsample the PhysioNet data to 250 hz\n",
        "2. Pad recordings to ~65 second length\n",
        "3. Normalize the recordings.\n"
      ],
      "metadata": {
        "id": "v1XRxBLDglRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation\n",
        "\n",
        "### Mount Google Drive"
      ],
      "metadata": {
        "id": "Cdto6uVnwRoS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DNqG2WkWgjH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f87fed3-dd24-42bf-b480-1bea4a27c1ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# You may also manually mount drive by clicking on folder icon in left sidebar\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clone code repo\n",
        "\n",
        "Assuming this notebook is run on Colab Pro, please clone our repo to the instance.\n",
        "\n",
        "Sample commands to run in Colab Pro Terminal:\n",
        "\n",
        "```bash\n",
        "$ cd /root\n",
        "# enter your username and github PAT\n",
        "$ git clone https://github.com/myles-i/DLH_TransferLearning.git\n",
        "$ cd DLH_TransferLearning\n",
        "```\n",
        "\n",
        "We will assume the code to run is on `master` branch.\n",
        "\n",
        "### Install dependencies\n",
        "\n",
        "We will install the dependencies from the `requirements.txt` file in the cloned repo."
      ],
      "metadata": {
        "id": "UDppmL8e9XEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "REPO = '/root/DLH_TransferLearning/'\n",
        "%cd $REPO"
      ],
      "metadata": {
        "id": "-zVHB1B1iDd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf8c6857-71c2-4eab-f50d-c2694ae5f9a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/DLH_TransferLearning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "hKEvhfLriULt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetuning\n",
        "\n",
        "Below we set up the paths to the:\n",
        "1. `DATA_DIR`: location where the input files are\n",
        "2. `JOB_DIR`: location where to save the output of the finetuning (model weights, history, etc.)"
      ],
      "metadata": {
        "id": "Fa1PrAkNxk53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ROOT = '/content/drive/MyDrive/DLHProject'\n",
        "DATA_DIR = PROJECT_ROOT + '/data'\n",
        "JOB_DIR = PROJECT_ROOT + '/jobs'\n",
        "! mkdir -p $JOB_DIR"
      ],
      "metadata": {
        "id": "M7jO-g3ziDbB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we set up the actual input and output paths relative to the `DATA_DIR` and `JOB_DIR`, respectively."
      ],
      "metadata": {
        "id": "oOPCMQFLilP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = DATA_DIR + '/physionet_finetune/physionet_train.pkl'\n",
        "test = DATA_DIR + '/physionet_finetune/physionet_test.pkl'\n",
        "job_name = 'finetune_baseline_65sec'\n",
        "job_dir = JOB_DIR + '/' + job_name\n",
        "\n",
        "print(f\"train: {train}\")\n",
        "print(f\"test: {test}\")\n",
        "print(f\"job_dir: {job_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atsAS6BlygqP",
        "outputId": "571dd636-6fb3-4c17-dd9d-1cdaa95c176d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: /content/drive/MyDrive/DLHProject/data/physionet_finetune/physionet_train.pkl\n",
            "test: /content/drive/MyDrive/DLHProject/data/physionet_finetune/physionet_test.pkl\n",
            "job_dir: /content/drive/MyDrive/DLHProject/jobs/finetune_baseline_65sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "job_dir = JOB_DIR + '/finetune_random_cnn_original_data_with_f1'\n",
        "train = DATA_DIR + '/physionet_finetune/physionet_train.pkl'\n",
        "test = DATA_DIR + '/physionet_finetune/physionet_test.pkl'\n",
        "\n",
        "print(f\"job_dir: {job_dir}\")\n",
        "print(f\"train: {train}\")\n",
        "print(f\"test: {test}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96dJL5rglyVq",
        "outputId": "67df3ce8-9d75-49bf-dde6-023e34946d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "job_dir: /content/drive/MyDrive/DLHProject/jobs/finetune_random_cnn_original_data_with_f1\n",
            "train: /content/drive/MyDrive/DLHProject/data/physionet_finetune/physionet_train.pkl\n",
            "test: /content/drive/MyDrive/DLHProject/data/physionet_finetune/physionet_test.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The paper authors provide us this script that will perform the finetuning. We will run it below and expand on the meaning of the parameter values.\n",
        "\n",
        "- `--val-size 0.0625`: This is the percentage of the train set size to set aside for the validation set.\n",
        "\n",
        "  Note that the PhysioNet data was already split 80-20 train-test. The paper uses 5 percent of the full dataset for validation. We get this via $0.0625 * 0.8 = 0.05$\n",
        "- `--val-metric \"f1\"`: Use macro F1 score to evaluate performance on validation set and to find the best model at each epoch.\n",
        "- `--arch \"resnet18\"` means we are using ResNet18 CNN.\n",
        "- `--batch-size 128` we found this value through trial and error that maximizes the utilization of the V100 GPU's 16 GB RAM.\n",
        "- `--epochs 200` This is taken from the paper.\n",
        "- `--seed 2024` this is the random seed used in splitting the train set into validation. This is for reproducibility.\n",
        "- `--verbose` this prints out the finetuning progress epoch by epoch.\n"
      ],
      "metadata": {
        "id": "WXvWxLRPyzc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!python -m finetuning.trainer \\\n",
        "--job-dir $job_dir \\\n",
        "--train $train \\\n",
        "--test $test \\\n",
        "--val-size 0.0625 \\\n",
        "--val-metric \"f1\" \\\n",
        "--arch \"resnet18\" \\\n",
        "--batch-size 128 \\\n",
        "--epochs 200 \\\n",
        "--seed 2024 \\\n",
        "--verbose"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQSa8mSljD39",
        "outputId": "209d9705-d446-4fef-8102-a5875bed805f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-12 05:41:52.757898: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-12 05:41:52.757948: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-12 05:41:52.759349: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-12 05:41:52.767136: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-12 05:41:53.963730: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Creating working directory in /content/drive/MyDrive/DLHProject/jobs/finetune_baseline_65sec\n",
            "Setting random state 2024\n",
            "Loading train data from /content/drive/MyDrive/DLHProject/data/physionet_finetune/physionet_train.pkl ...\n",
            "Split data into train 93.74% and validation 6.26%\n",
            "Loading test data from /content/drive/MyDrive/DLHProject/data/physionet_finetune/physionet_test.pkl ...\n",
            "Train data shape: (6395, 16384, 1)\n",
            "2024-04-12 05:42:04.394933: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-12 05:42:04.452938: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-12 05:42:04.453266: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-12 05:42:04.454058: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-12 05:42:04.454329: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-12 05:42:04.454592: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-12 05:42:04.618837: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-12 05:42:04.619195: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-12 05:42:04.619358: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2024-04-12 05:42:04.619491: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-12 05:42:04.619647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14792 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "2024-04-12 05:42:04.620586: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 838205440 exceeds 10% of free system memory.\n",
            "2024-04-12 05:42:05.472116: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 838205440 exceeds 10% of free system memory.\n",
            "Building model ...\n",
            "# model parameters: 4,494,532\n",
            "2024-04-12 05:42:08.649335: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 838205440 exceeds 10% of free system memory.\n",
            "2024-04-12 05:42:09.391076: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 838205440 exceeds 10% of free system memory.\n",
            "2024-04-12 05:42:10.193557: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 838205440 exceeds 10% of free system memory.\n",
            "Epoch 1/200\n",
            "2024-04-12 05:42:18.859344: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8906\n",
            "2024-04-12 05:42:24.088432: I external/local_xla/xla/service/service.cc:168] XLA service 0x7a0e28304990 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2024-04-12 05:42:24.088564: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
            "2024-04-12 05:42:24.096986: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1712900544.213683    6932 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00001: f1 improved from -inf to 0.30850, saving model to /content/drive/MyDrive/DLHProject/jobs/finetune_baseline_65sec/best_model.weights\n",
            "50/50 - 47s - loss: 0.9759 - acc: 0.5875 - val_loss: 0.8674 - val_acc: 0.6230 - f1: 0.3085 - 47s/epoch - 930ms/step\n",
            "Epoch 2/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00002: f1 improved from 0.30850 to 0.45906, saving model to /content/drive/MyDrive/DLHProject/jobs/finetune_baseline_65sec/best_model.weights\n",
            "50/50 - 22s - loss: 0.8169 - acc: 0.6529 - val_loss: 0.9330 - val_acc: 0.6440 - f1: 0.4591 - 22s/epoch - 443ms/step\n",
            "Epoch 3/200\n",
            "4/4 [==============================] - 1s 103ms/step\n",
            "\n",
            "Epoch 00003: f1 improved from 0.45906 to 0.58635, saving model to /content/drive/MyDrive/DLHProject/jobs/finetune_baseline_65sec/best_model.weights\n",
            "50/50 - 23s - loss: 0.7294 - acc: 0.6913 - val_loss: 0.6948 - val_acc: 0.7119 - f1: 0.5864 - 23s/epoch - 455ms/step\n",
            "Epoch 4/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00004: f1 improved from 0.58635 to 0.66620, saving model to /content/drive/MyDrive/DLHProject/jobs/finetune_baseline_65sec/best_model.weights\n",
            "50/50 - 22s - loss: 0.6589 - acc: 0.7240 - val_loss: 0.7041 - val_acc: 0.7447 - f1: 0.6662 - 22s/epoch - 436ms/step\n",
            "Epoch 5/200\n",
            "4/4 [==============================] - 1s 100ms/step\n",
            "\n",
            "Epoch 00005: f1 (0.62543) did not improve from 0.66620\n",
            "50/50 - 21s - loss: 0.6298 - acc: 0.7359 - val_loss: 0.7648 - val_acc: 0.6792 - f1: 0.6254 - 21s/epoch - 425ms/step\n",
            "Epoch 6/200\n",
            "4/4 [==============================] - 1s 101ms/step\n",
            "\n",
            "Epoch 00006: f1 (0.66277) did not improve from 0.66620\n",
            "50/50 - 22s - loss: 0.6208 - acc: 0.7443 - val_loss: 0.6155 - val_acc: 0.7611 - f1: 0.6628 - 22s/epoch - 439ms/step\n",
            "Epoch 7/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00007: f1 (0.65114) did not improve from 0.66620\n",
            "50/50 - 21s - loss: 0.5845 - acc: 0.7679 - val_loss: 0.6073 - val_acc: 0.7564 - f1: 0.6511 - 21s/epoch - 424ms/step\n",
            "Epoch 8/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00008: f1 improved from 0.66620 to 0.74666, saving model to /content/drive/MyDrive/DLHProject/jobs/finetune_baseline_65sec/best_model.weights\n",
            "50/50 - 22s - loss: 0.5576 - acc: 0.7764 - val_loss: 0.6697 - val_acc: 0.7939 - f1: 0.7467 - 22s/epoch - 437ms/step\n",
            "Epoch 9/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00009: f1 (0.73201) did not improve from 0.74666\n",
            "50/50 - 22s - loss: 0.5322 - acc: 0.7914 - val_loss: 0.6407 - val_acc: 0.7986 - f1: 0.7320 - 22s/epoch - 432ms/step\n",
            "Epoch 10/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00010: f1 (0.74545) did not improve from 0.74666\n",
            "50/50 - 21s - loss: 0.5203 - acc: 0.7945 - val_loss: 0.5960 - val_acc: 0.8126 - f1: 0.7454 - 21s/epoch - 424ms/step\n",
            "Epoch 11/200\n",
            "4/4 [==============================] - 0s 98ms/step\n",
            "\n",
            "Epoch 00011: f1 (0.57849) did not improve from 0.74666\n",
            "50/50 - 21s - loss: 0.5175 - acc: 0.7958 - val_loss: 0.6453 - val_acc: 0.7635 - f1: 0.5785 - 21s/epoch - 425ms/step\n",
            "Epoch 12/200\n",
            "4/4 [==============================] - 1s 103ms/step\n",
            "\n",
            "Epoch 00012: f1 (0.69777) did not improve from 0.74666\n",
            "50/50 - 22s - loss: 0.5009 - acc: 0.7980 - val_loss: 0.5863 - val_acc: 0.8056 - f1: 0.6978 - 22s/epoch - 434ms/step\n",
            "Epoch 13/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00013: f1 (0.67462) did not improve from 0.74666\n",
            "50/50 - 21s - loss: 0.4710 - acc: 0.8144 - val_loss: 0.7925 - val_acc: 0.7494 - f1: 0.6746 - 21s/epoch - 424ms/step\n",
            "Epoch 14/200\n",
            "4/4 [==============================] - 1s 96ms/step\n",
            "\n",
            "Epoch 00014: f1 (0.71171) did not improve from 0.74666\n",
            "50/50 - 21s - loss: 0.4793 - acc: 0.8156 - val_loss: 0.5696 - val_acc: 0.8103 - f1: 0.7117 - 21s/epoch - 424ms/step\n",
            "Epoch 15/200\n",
            "4/4 [==============================] - 1s 103ms/step\n",
            "\n",
            "Epoch 00015: f1 (0.71449) did not improve from 0.74666\n",
            "50/50 - 22s - loss: 0.4782 - acc: 0.8134 - val_loss: 0.5658 - val_acc: 0.8080 - f1: 0.7145 - 22s/epoch - 433ms/step\n",
            "Epoch 16/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00016: f1 improved from 0.74666 to 0.76454, saving model to /content/drive/MyDrive/DLHProject/jobs/finetune_baseline_65sec/best_model.weights\n",
            "50/50 - 22s - loss: 0.4444 - acc: 0.8269 - val_loss: 0.5381 - val_acc: 0.8361 - f1: 0.7645 - 22s/epoch - 436ms/step\n",
            "Epoch 17/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00017: f1 (0.73910) did not improve from 0.76454\n",
            "50/50 - 21s - loss: 0.4576 - acc: 0.8210 - val_loss: 0.5269 - val_acc: 0.8220 - f1: 0.7391 - 21s/epoch - 425ms/step\n",
            "Epoch 18/200\n",
            "4/4 [==============================] - 1s 95ms/step\n",
            "\n",
            "Epoch 00018: f1 (0.69787) did not improve from 0.76454\n",
            "50/50 - 22s - loss: 0.4327 - acc: 0.8346 - val_loss: 0.5802 - val_acc: 0.8056 - f1: 0.6979 - 22s/epoch - 434ms/step\n",
            "Epoch 19/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00019: f1 (0.69832) did not improve from 0.76454\n",
            "50/50 - 21s - loss: 0.4400 - acc: 0.8285 - val_loss: 0.6398 - val_acc: 0.7963 - f1: 0.6983 - 21s/epoch - 424ms/step\n",
            "Epoch 20/200\n",
            "4/4 [==============================] - 0s 100ms/step\n",
            "\n",
            "Epoch 00020: f1 (0.72975) did not improve from 0.76454\n",
            "50/50 - 21s - loss: 0.4183 - acc: 0.8405 - val_loss: 0.5873 - val_acc: 0.8173 - f1: 0.7298 - 21s/epoch - 425ms/step\n",
            "Epoch 21/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00021: f1 improved from 0.76454 to 0.77078, saving model to /content/drive/MyDrive/DLHProject/jobs/finetune_baseline_65sec/best_model.weights\n",
            "50/50 - 23s - loss: 0.3978 - acc: 0.8525 - val_loss: 0.6864 - val_acc: 0.8361 - f1: 0.7708 - 23s/epoch - 450ms/step\n",
            "Epoch 22/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00022: f1 (0.72045) did not improve from 0.77078\n",
            "50/50 - 21s - loss: 0.4170 - acc: 0.8397 - val_loss: 0.5441 - val_acc: 0.8009 - f1: 0.7204 - 21s/epoch - 425ms/step\n",
            "Epoch 23/200\n",
            "4/4 [==============================] - 1s 103ms/step\n",
            "\n",
            "Epoch 00023: f1 (0.76009) did not improve from 0.77078\n",
            "50/50 - 22s - loss: 0.3911 - acc: 0.8518 - val_loss: 0.5069 - val_acc: 0.8478 - f1: 0.7601 - 22s/epoch - 430ms/step\n",
            "Epoch 24/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00024: f1 improved from 0.77078 to 0.78049, saving model to /content/drive/MyDrive/DLHProject/jobs/finetune_baseline_65sec/best_model.weights\n",
            "50/50 - 22s - loss: 0.3790 - acc: 0.8530 - val_loss: 0.5656 - val_acc: 0.8384 - f1: 0.7805 - 22s/epoch - 440ms/step\n",
            "Epoch 25/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00025: f1 (0.76361) did not improve from 0.78049\n",
            "50/50 - 21s - loss: 0.3809 - acc: 0.8588 - val_loss: 0.5935 - val_acc: 0.8173 - f1: 0.7636 - 21s/epoch - 424ms/step\n",
            "Epoch 26/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00026: f1 (0.77125) did not improve from 0.78049\n",
            "50/50 - 22s - loss: 0.3763 - acc: 0.8549 - val_loss: 0.5149 - val_acc: 0.8595 - f1: 0.7712 - 22s/epoch - 433ms/step\n",
            "Epoch 27/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00027: f1 (0.74977) did not improve from 0.78049\n",
            "50/50 - 21s - loss: 0.3508 - acc: 0.8669 - val_loss: 0.5487 - val_acc: 0.8290 - f1: 0.7498 - 21s/epoch - 424ms/step\n",
            "Epoch 28/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00028: f1 improved from 0.78049 to 0.79478, saving model to /content/drive/MyDrive/DLHProject/jobs/finetune_baseline_65sec/best_model.weights\n",
            "50/50 - 22s - loss: 0.3388 - acc: 0.8737 - val_loss: 0.6118 - val_acc: 0.8525 - f1: 0.7948 - 22s/epoch - 436ms/step\n",
            "Epoch 29/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00029: f1 (0.71193) did not improve from 0.79478\n",
            "50/50 - 22s - loss: 0.3345 - acc: 0.8716 - val_loss: 0.6784 - val_acc: 0.8080 - f1: 0.7119 - 22s/epoch - 432ms/step\n",
            "Epoch 30/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00030: f1 (0.69217) did not improve from 0.79478\n",
            "50/50 - 21s - loss: 0.3203 - acc: 0.8782 - val_loss: 0.6227 - val_acc: 0.8056 - f1: 0.6922 - 21s/epoch - 424ms/step\n",
            "Epoch 31/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00031: f1 (0.72033) did not improve from 0.79478\n",
            "50/50 - 21s - loss: 0.3109 - acc: 0.8851 - val_loss: 0.6370 - val_acc: 0.8290 - f1: 0.7203 - 21s/epoch - 424ms/step\n",
            "Epoch 32/200\n",
            "4/4 [==============================] - 1s 102ms/step\n",
            "\n",
            "Epoch 00032: f1 (0.74689) did not improve from 0.79478\n",
            "50/50 - 22s - loss: 0.2957 - acc: 0.8918 - val_loss: 0.6006 - val_acc: 0.8337 - f1: 0.7469 - 22s/epoch - 432ms/step\n",
            "Epoch 33/200\n",
            "4/4 [==============================] - 1s 100ms/step\n",
            "\n",
            "Epoch 00033: f1 improved from 0.79478 to 0.80691, saving model to /content/drive/MyDrive/DLHProject/jobs/finetune_baseline_65sec/best_model.weights\n",
            "50/50 - 22s - loss: 0.2835 - acc: 0.8930 - val_loss: 0.5461 - val_acc: 0.8548 - f1: 0.8069 - 22s/epoch - 435ms/step\n",
            "Epoch 34/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00034: f1 (0.71958) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.2909 - acc: 0.8930 - val_loss: 0.7309 - val_acc: 0.8150 - f1: 0.7196 - 21s/epoch - 430ms/step\n",
            "Epoch 35/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00035: f1 (0.73006) did not improve from 0.80691\n",
            "50/50 - 22s - loss: 0.2605 - acc: 0.9043 - val_loss: 0.8133 - val_acc: 0.8009 - f1: 0.7301 - 22s/epoch - 431ms/step\n",
            "Epoch 36/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00036: f1 (0.72677) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.3505 - acc: 0.8710 - val_loss: 0.6603 - val_acc: 0.8126 - f1: 0.7268 - 21s/epoch - 424ms/step\n",
            "Epoch 37/200\n",
            "4/4 [==============================] - 1s 98ms/step\n",
            "\n",
            "Epoch 00037: f1 (0.68407) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.3138 - acc: 0.8813 - val_loss: 0.5974 - val_acc: 0.8103 - f1: 0.6841 - 21s/epoch - 425ms/step\n",
            "Epoch 38/200\n",
            "4/4 [==============================] - 1s 105ms/step\n",
            "\n",
            "Epoch 00038: f1 (0.71306) did not improve from 0.80691\n",
            "50/50 - 22s - loss: 0.2354 - acc: 0.9173 - val_loss: 0.5795 - val_acc: 0.8244 - f1: 0.7131 - 22s/epoch - 433ms/step\n",
            "Epoch 39/200\n",
            "4/4 [==============================] - 1s 100ms/step\n",
            "\n",
            "Epoch 00039: f1 (0.68010) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.2137 - acc: 0.9231 - val_loss: 0.7772 - val_acc: 0.8150 - f1: 0.6801 - 21s/epoch - 426ms/step\n",
            "Epoch 40/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00040: f1 (0.75770) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.2109 - acc: 0.9235 - val_loss: 0.7552 - val_acc: 0.8337 - f1: 0.7577 - 21s/epoch - 430ms/step\n",
            "Epoch 41/200\n",
            "4/4 [==============================] - 1s 100ms/step\n",
            "\n",
            "Epoch 00041: f1 (0.76760) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.1833 - acc: 0.9359 - val_loss: 0.7061 - val_acc: 0.8361 - f1: 0.7676 - 21s/epoch - 424ms/step\n",
            "Epoch 42/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00042: f1 (0.69203) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.1741 - acc: 0.9365 - val_loss: 0.8806 - val_acc: 0.8080 - f1: 0.6920 - 21s/epoch - 425ms/step\n",
            "Epoch 43/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00043: f1 (0.76766) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.1700 - acc: 0.9393 - val_loss: 0.8399 - val_acc: 0.8407 - f1: 0.7677 - 21s/epoch - 424ms/step\n",
            "Epoch 44/200\n",
            "4/4 [==============================] - 1s 97ms/step\n",
            "\n",
            "Epoch 00044: f1 (0.69184) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.1566 - acc: 0.9443 - val_loss: 0.8925 - val_acc: 0.7658 - f1: 0.6918 - 21s/epoch - 429ms/step\n",
            "Epoch 45/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00045: f1 (0.72840) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.1508 - acc: 0.9489 - val_loss: 0.8569 - val_acc: 0.8173 - f1: 0.7284 - 21s/epoch - 423ms/step\n",
            "Epoch 46/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00046: f1 (0.72018) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.1256 - acc: 0.9564 - val_loss: 0.9320 - val_acc: 0.8267 - f1: 0.7202 - 21s/epoch - 425ms/step\n",
            "Epoch 47/200\n",
            "4/4 [==============================] - 1s 102ms/step\n",
            "\n",
            "Epoch 00047: f1 (0.71120) did not improve from 0.80691\n",
            "50/50 - 22s - loss: 0.0958 - acc: 0.9701 - val_loss: 0.8895 - val_acc: 0.8126 - f1: 0.7112 - 22s/epoch - 439ms/step\n",
            "Epoch 48/200\n",
            "4/4 [==============================] - 1s 100ms/step\n",
            "\n",
            "Epoch 00048: f1 (0.69678) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.0850 - acc: 0.9715 - val_loss: 0.9652 - val_acc: 0.8009 - f1: 0.6968 - 21s/epoch - 423ms/step\n",
            "Epoch 49/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00049: f1 (0.71695) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.1140 - acc: 0.9611 - val_loss: 0.9887 - val_acc: 0.8009 - f1: 0.7169 - 21s/epoch - 425ms/step\n",
            "Epoch 50/200\n",
            "4/4 [==============================] - 1s 103ms/step\n",
            "\n",
            "Epoch 00050: f1 (0.68701) did not improve from 0.80691\n",
            "50/50 - 22s - loss: 0.1319 - acc: 0.9553 - val_loss: 0.7990 - val_acc: 0.8103 - f1: 0.6870 - 22s/epoch - 440ms/step\n",
            "Epoch 51/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00051: f1 (0.75152) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.0827 - acc: 0.9706 - val_loss: 1.0270 - val_acc: 0.8384 - f1: 0.7515 - 21s/epoch - 424ms/step\n",
            "Epoch 52/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00052: f1 (0.73146) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.0552 - acc: 0.9844 - val_loss: 0.9527 - val_acc: 0.8290 - f1: 0.7315 - 21s/epoch - 424ms/step\n",
            "Epoch 53/200\n",
            "4/4 [==============================] - 1s 101ms/step\n",
            "\n",
            "Epoch 00053: f1 (0.73550) did not improve from 0.80691\n",
            "50/50 - 22s - loss: 0.0698 - acc: 0.9775 - val_loss: 1.1833 - val_acc: 0.8173 - f1: 0.7355 - 22s/epoch - 433ms/step\n",
            "Epoch 54/200\n",
            "4/4 [==============================] - 1s 100ms/step\n",
            "\n",
            "Epoch 00054: f1 (0.72570) did not improve from 0.80691\n",
            "50/50 - 22s - loss: 0.0646 - acc: 0.9812 - val_loss: 1.2795 - val_acc: 0.8126 - f1: 0.7257 - 22s/epoch - 430ms/step\n",
            "Epoch 55/200\n",
            "4/4 [==============================] - 1s 100ms/step\n",
            "\n",
            "Epoch 00055: f1 (0.73050) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.0859 - acc: 0.9719 - val_loss: 0.9503 - val_acc: 0.8220 - f1: 0.7305 - 21s/epoch - 424ms/step\n",
            "Epoch 56/200\n",
            "4/4 [==============================] - 1s 104ms/step\n",
            "\n",
            "Epoch 00056: f1 (0.73774) did not improve from 0.80691\n",
            "50/50 - 22s - loss: 0.0953 - acc: 0.9694 - val_loss: 0.8294 - val_acc: 0.8244 - f1: 0.7377 - 22s/epoch - 438ms/step\n",
            "Epoch 57/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00057: f1 (0.70287) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.0699 - acc: 0.9775 - val_loss: 1.0142 - val_acc: 0.8126 - f1: 0.7029 - 21s/epoch - 424ms/step\n",
            "Epoch 58/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00058: f1 (0.73112) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.0398 - acc: 0.9883 - val_loss: 0.9861 - val_acc: 0.8173 - f1: 0.7311 - 21s/epoch - 425ms/step\n",
            "Epoch 59/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00059: f1 (0.73693) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.0305 - acc: 0.9919 - val_loss: 1.1198 - val_acc: 0.8220 - f1: 0.7369 - 21s/epoch - 425ms/step\n",
            "Epoch 60/200\n",
            "4/4 [==============================] - 1s 101ms/step\n",
            "\n",
            "Epoch 00060: f1 (0.72582) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.0173 - acc: 0.9956 - val_loss: 1.1190 - val_acc: 0.8103 - f1: 0.7258 - 21s/epoch - 424ms/step\n",
            "Epoch 61/200\n",
            "4/4 [==============================] - 1s 103ms/step\n",
            "\n",
            "Epoch 00061: f1 (0.73445) did not improve from 0.80691\n",
            "50/50 - 22s - loss: 0.0126 - acc: 0.9966 - val_loss: 1.1615 - val_acc: 0.8314 - f1: 0.7344 - 22s/epoch - 438ms/step\n",
            "Epoch 62/200\n",
            "4/4 [==============================] - 1s 98ms/step\n",
            "\n",
            "Epoch 00062: f1 (0.69016) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.0160 - acc: 0.9962 - val_loss: 1.2056 - val_acc: 0.7939 - f1: 0.6902 - 21s/epoch - 425ms/step\n",
            "Epoch 63/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00063: f1 (0.73224) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.0128 - acc: 0.9972 - val_loss: 1.0480 - val_acc: 0.8267 - f1: 0.7322 - 21s/epoch - 425ms/step\n",
            "Epoch 64/200\n",
            "4/4 [==============================] - 1s 102ms/step\n",
            "\n",
            "Epoch 00064: f1 (0.75470) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.0122 - acc: 0.9975 - val_loss: 1.1803 - val_acc: 0.8337 - f1: 0.7547 - 21s/epoch - 428ms/step\n",
            "Epoch 65/200\n",
            "4/4 [==============================] - 1s 100ms/step\n",
            "\n",
            "Epoch 00065: f1 (0.74884) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.0134 - acc: 0.9967 - val_loss: 1.0994 - val_acc: 0.8267 - f1: 0.7488 - 21s/epoch - 427ms/step\n",
            "Epoch 66/200\n",
            "4/4 [==============================] - 1s 97ms/step\n",
            "\n",
            "Epoch 00066: f1 (0.72194) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.0326 - acc: 0.9894 - val_loss: 1.1854 - val_acc: 0.8103 - f1: 0.7219 - 21s/epoch - 424ms/step\n",
            "Epoch 67/200\n",
            "4/4 [==============================] - 1s 100ms/step\n",
            "\n",
            "Epoch 00067: f1 (0.72370) did not improve from 0.80691\n",
            "50/50 - 22s - loss: 0.0258 - acc: 0.9914 - val_loss: 1.3576 - val_acc: 0.8150 - f1: 0.7237 - 22s/epoch - 439ms/step\n",
            "Epoch 68/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00068: f1 (0.72912) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.0578 - acc: 0.9822 - val_loss: 0.9562 - val_acc: 0.8314 - f1: 0.7291 - 21s/epoch - 424ms/step\n",
            "Epoch 69/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00069: f1 (0.74308) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.0866 - acc: 0.9714 - val_loss: 1.0178 - val_acc: 0.8290 - f1: 0.7431 - 21s/epoch - 424ms/step\n",
            "Epoch 70/200\n",
            "4/4 [==============================] - 1s 102ms/step\n",
            "\n",
            "Epoch 00070: f1 (0.72892) did not improve from 0.80691\n",
            "50/50 - 22s - loss: 0.0587 - acc: 0.9781 - val_loss: 1.2052 - val_acc: 0.8103 - f1: 0.7289 - 22s/epoch - 432ms/step\n",
            "Epoch 71/200\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00071: f1 (0.70689) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.0350 - acc: 0.9903 - val_loss: 1.0770 - val_acc: 0.8009 - f1: 0.7069 - 21s/epoch - 424ms/step\n",
            "Epoch 72/200\n",
            "4/4 [==============================] - 1s 96ms/step\n",
            "\n",
            "Epoch 00072: f1 (0.74554) did not improve from 0.80691\n",
            "50/50 - 21s - loss: 0.0180 - acc: 0.9953 - val_loss: 1.0725 - val_acc: 0.8337 - f1: 0.7455 - 21s/epoch - 425ms/step\n",
            "Epoch 73/200\n",
            "4/4 [==============================] - 1s 102ms/step\n",
            "\n",
            "Epoch 00073: f1 (0.72812) did not improve from 0.80691\n",
            "50/50 - 22s - loss: 0.0098 - acc: 0.9973 - val_loss: 1.1809 - val_acc: 0.8126 - f1: 0.7281 - 22s/epoch - 438ms/step\n",
            "Epoch 73: early stopping\n",
            "Loading the best weights from file /content/drive/MyDrive/DLHProject/jobs/finetune_baseline_65sec/best_model.weights ...\n",
            "Predicting training data ...\n",
            "50/50 [==============================] - 6s 102ms/step\n",
            "Predicting validation data ...\n",
            "4/4 [==============================] - 0s 99ms/step\n",
            "Predicting test data ...\n",
            "14/14 [==============================] - 2s 142ms/step\n",
            "CPU times: user 11 s, sys: 1.29 s, total: 12.3 s\n",
            "Wall time: 30min 20s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It took 30m 20s to run 72 epochs, as the paper authors have set up early stopping to end training if the loss on validation set does not decrease for more than 50 epochs.\n",
        "\n"
      ],
      "metadata": {
        "id": "QdSDq1xU1UHz"
      }
    }
  ]
}