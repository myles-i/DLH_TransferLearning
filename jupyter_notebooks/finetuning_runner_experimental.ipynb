{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eDeQUIx2AdP"
      },
      "source": [
        "### Clone Git repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoHTm9Fz2BM9",
        "outputId": "d59f45ed-2e45-434d-bee0-1279790fc4a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/root\n",
            "Cloning into 'DLH_TransferLearning'...\n",
            "remote: Enumerating objects: 599, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 599 (delta 41), reused 44 (delta 21), pack-reused 527\u001b[K\n",
            "Receiving objects: 100% (599/599), 4.33 MiB | 10.59 MiB/s, done.\n",
            "Resolving deltas: 100% (350/350), done.\n",
            "/root/DLH_TransferLearning\n"
          ]
        }
      ],
      "source": [
        "%cd /root\n",
        "! git clone https://github.com/myles-i/DLH_TransferLearning.git\n",
        "%cd DLH_TransferLearning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPw1-7Fw38el"
      },
      "source": [
        "### Install dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wLPArTJF4CbC"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvYiiuY919T5"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LGFwWzAwAux",
        "outputId": "016a3087-6a8c-45f8-a858-23f22e807eec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "913MhsuXxIXx"
      },
      "source": [
        "### Determine path to data and job directories on Drive\n",
        "\n",
        "Example for me:\n",
        "\n",
        "`/content/drive/MyDrive/DLHProject`\n",
        "\n",
        "where `DLHProject` is the name of my shortcut for the shared folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dzwyLJsp1Nin"
      },
      "outputs": [],
      "source": [
        "# setup top level directory paths\n",
        "PROJECT_DIR = '/content/drive/MyDrive/DLHProject'\n",
        "DATA_DIR = PROJECT_DIR + '/data'\n",
        "JOB_DIR = PROJECT_DIR + '/jobs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VbDytx2jWLD",
        "outputId": "6b016d9e-c7fe-4cae-d092-3abcf25550ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "seq 10 10 100 | xargs -P 1 -I {} python finetune_runner.py \\\n",
            "--arch resnet18_2d \\\n",
            "--weights-type random \\\n",
            "--weights-file  \\\n",
            "--job-base-dir /content/drive/MyDrive/DLHProject/jobs/spectrogram/finetuning/min_normalization_16epochs_to_20percent \\\n",
            "--train /content/drive/MyDrive/DLHProject/data/physionet_finetune_spectrogram/physionet_train.pkl \\\n",
            "--test /content/drive/MyDrive/DLHProject/data/physionet_finetune_spectrogram/physionet_test.pkl \\\n",
            "--batch-size 128 \\\n",
            "--epochs 200 \\\n",
            "--seed '{}' \\\n",
            "--dryrun False \\\n",
            "\n",
            "Configured command:\n",
            "python -u -m finetuning.trainer --job-dir /content/drive/MyDrive/DLHProject/jobs/spectrogram/finetuning/min_normalization_16epochs_to_20percent/finetune__random_seed10 --train /content/drive/MyDrive/DLHProject/data/physionet_finetune_spectrogram/physionet_train.pkl --test /content/drive/MyDrive/DLHProject/data/physionet_finetune_spectrogram/physionet_test.pkl --val-size 0.0625 --val-metric f1 --arch resnet18_2d --batch-size 128 --epochs 200 --seed 10\n",
            "========================================\n",
            "2024-04-26 15:26:04.184048: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-04-26 15:26:04.238168: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-26 15:26:04.238219: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-26 15:26:04.240278: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-26 15:26:04.249048: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-26 15:26:05.323575: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Creating working directory in /content/drive/MyDrive/DLHProject/jobs/spectrogram/finetuning/min_normalization_16epochs_to_20percent/finetune__random_seed10\n",
            "Setting random state 10\n",
            "Loading train data from /content/drive/MyDrive/DLHProject/data/physionet_finetune_spectrogram/physionet_train.pkl ...\n",
            "Split data into train 93.74% and validation 6.26%\n",
            "Loading test data from /content/drive/MyDrive/DLHProject/data/physionet_finetune_spectrogram/physionet_test.pkl ...\n",
            "Train data shape: (6395, 128, 512, 1)\n",
            "2024-04-26 15:26:33.141120: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2024-04-26 15:26:33.141340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20972 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
            "Building model ...\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method ResnetBlock.call of <transplant.modules.resnet2d.ResnetBlock object at 0x7bde428dcd00>> and will run it as-is.\n",
            "Cause: mangled names are not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "# model parameters: 11,186,692\n",
            "Epoch 1/200\n",
            "2024-04-26 15:26:50.311662: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8906\n",
            "2024-04-26 15:26:54.394947: I external/local_xla/xla/service/service.cc:168] XLA service 0x7bdba9d8ab30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2024-04-26 15:26:54.395003: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA L4, Compute Capability 8.9\n",
            "2024-04-26 15:26:54.401749: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1714145214.522872   10222 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "dryrun = True # change this to False once you have verified everything looks good\n",
        "\n",
        "# finetune/pretraining data paths (can vary depending on model used)\n",
        "arch = \"resnet18_2d\"\n",
        "batch_size = 128 #for ResNet2D, batch size needs to be 64 on V100\n",
        "EXPERIMENTAL_BASE_DIR = DATA_DIR + \"/physionet_finetune_spectrogram/experimental\"\n",
        "PHYSIONET_TRAIN = DATA_DIR + \"/physionet_finetune_spectrogram/physionet_train.pkl\"\n",
        "PHYSIONET_TEST = DATA_DIR + \"/physionet_finetune_spectrogram/physionet_test.pkl\"\n",
        "PRETRAIN_BASE =JOB_DIR + \"/spectrogram/pretraining/min_normalization_16epochs_to_20percent\"\n",
        "FINETUNE_BASE =JOB_DIR + \"/spectrogram/finetuning/min_normalization_16epochs_to_20percent\"\n",
        "os.makedirs(FINETUNE_BASE, exist_ok=True)\n",
        "\n",
        "# define which combinations of runs to do\n",
        "# make a list of all directories in DATA_DIR + \"/physionet_finetune_spectrogram/experimental\n",
        "DIRs = os.listdir(EXPERIMENTAL_BASE_DIR)\n",
        "print(DIRs)\n",
        "# Loop through each weight type and corresponding weight file\n",
        "for subdir in DIRs:\n",
        "    # Assert that the required datasets exist\n",
        "    PHYSIONET_TRAIN = EXPERIMENTAL_BASE_DIR + f\"/{subdir}/physionet_train.pkl\"\n",
        "    PHYSIONET_TEST = EXPERIMENTAL_BASE_DIR + f\"/{subdir}/physionet_test.pkl\"\n",
        "\n",
        "    assert os.path.exists(PHYSIONET_TRAIN), f\"PHYSIONET_TRAIN does not exist: {PHYSIONET_TRAIN}\"\n",
        "    assert os.path.exists(PHYSIONET_TEST), f\"PHYSIONET_TEST does not exist: {PHYSIONET_TEST}\"\n",
        "\n",
        "    # define output dir\n",
        "    FINETUNE_OUTPUT = FINETUNE_BASE + f\"/{subdir}\"\n",
        "    os.makedirs(FINETUNE_OUTPUT, exist_ok=True)\n",
        "\n",
        "    # Define the command to run\n",
        "    command = (\n",
        "        f\"python finetune_runner.py \\\\\\n\"\n",
        "        f\"--arch {arch} \\\\\\n\"\n",
        "        f\"--weights-type random\\\\\\n\"\n",
        "        f\"--job-base-dir {FINETUNE_OUTPUT} \\\\\\n\"\n",
        "        f\"--train {PHYSIONET_TRAIN} \\\\\\n\"\n",
        "        f\"--test {PHYSIONET_TEST} \\\\\\n\"\n",
        "        f\"--batch-size 128 \\\\\\n\"\n",
        "        f\"--epochs 200 \\\\\\n\"\n",
        "        f\"--seed '10' \\\\\\n\"\n",
        "        f\"--dryrun {dryrun} \\\\\\n\"\n",
        "    )\n",
        "    # Execute the command and stream the output here\n",
        "    print(command)\n",
        "    ! {command}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KCjMVjoHtBkZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
