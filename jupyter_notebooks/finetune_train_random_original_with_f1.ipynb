{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune: train on random init CNN and original prepared dataset Version 2!\n",
        "\n",
        "Copy of `jupyter_notebooks/finetune_train_random_original.ipynb`,\n",
        "\n",
        "but now we try adding macro f1 metric to the model metrics to be evaluated on the validation set at epoch end. The goal is to get macro f1 scores into the output history.csv so that we can plot the validation score by epoch curve. This would be our attempt to reproduce Figure 3 in the paper.\n",
        "\n",
        "## Prereqs\n",
        "\n",
        "Local git repo checked out to `dhxu2-f1-metric` branch (temporary, will remove this prereq once we validate this f1 change)\n",
        "\n",
        "---\n",
        "\n",
        "Original prepared dataset is the code in `finetuning/readme.md` which samples at 250 hz, 65 seconds.\n",
        "\n",
        "Now we try training on an uninitialized CNN.\n",
        "\n",
        "We do this by *not* passing in the `--weights-file` parameter to `finetuning.trainer`.\n",
        "\n",
        "It's not quite clear whether we need to explicitly fill the CNN with randomized weights or if the CNN network weights themselves are already randomized when no model weights are loaded into it."
      ],
      "metadata": {
        "id": "v1XRxBLDglRi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DNqG2WkWgjH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a3d841e-ff3f-4083-e7df-938321b0de43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# You may also manually mount drive by clicking on folder icon in left sidebar\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ROOT = '/content/drive/MyDrive/DLHProject'"
      ],
      "metadata": {
        "id": "M7jO-g3ziDbB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have colab pro, clone our repo to /root directory."
      ],
      "metadata": {
        "id": "UDppmL8e9XEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REPO = PROJECT_ROOT + '/Danielgitrepo'\n",
        "# Below is if you have colab pro\n",
        "REPO = '/root/DLH_TransferLearning/'"
      ],
      "metadata": {
        "id": "-zVHB1B1iDd3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $REPO"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL9h8ntdiDhp",
        "outputId": "5f2d9e36-754b-47db-978a-c5d7c0c8b9f4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/DLH_TransferLearning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "hKEvhfLriULt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = PROJECT_ROOT + '/data'"
      ],
      "metadata": {
        "id": "J_SX1ghHiUH0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls $DATA_DIR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXzD5t0riUEl",
        "outputId": "64363022-b95d-4930-8762-10db375b49bf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "icentia11k\t\t     icentia11k_subset_unzipped\t\t physionet_finetune\n",
            "icentia11k_corrupted\t     physionet\t\t\t\t physionet_preread\n",
            "icentia11k_subset\t     physionet_250hz_15000pad_norm_True  session_checkpoint.dat\n",
            "icentia11k_subset_corrupted  physionet_data.zip\t\t\t temp.torrent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we run the finetuning trainer, we should prepare job output directory. We make a sister `jobs` directory to the data directory.\n",
        "\n",
        "It should be noted that finetune trainer should also create the jobs directory for you by Line49:\n",
        "\n",
        "```python\n",
        "os.makedirs(str(args.job_dir), exist_ok=True)\n",
        "```\n",
        "\n",
        "But I think it is a good practice to just set up the directory structure yourself instead of just assuming the code will do it all for you."
      ],
      "metadata": {
        "id": "oOPCMQFLilP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "JOB_DIR = PROJECT_ROOT + '/jobs'\n",
        "# already created in the precursor notebook\n",
        "# ! mkdir -p $JOB_DIR"
      ],
      "metadata": {
        "id": "dJN-qWu-iUBv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls $PROJECT_ROOT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hQjn1eUinOD",
        "outputId": "1d5d6f30-5c32-4727-c6e8-635c01c090a9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Archive\t\t\t Mylesgitrepo\n",
            " Danielgitrepo\t\t\t'Notes from last yearâ€™s Class.gdoc'\n",
            " data\t\t\t\t proposal\n",
            " data_partial_download\t\t Sched:Daniel:M-F1700+,Sa-SuAllday:Vacay:None.gdoc\n",
            " DL4H_Team_1\t\t\t Sched:Myles:M-Su0300-1900:VacayMar01-Mar17.gdoc\n",
            " DownloadData.ipynb\t\t Sched:Ted:M-F1700+,Sa-Su1500+:Vacay:Mar31-Apr06.gdoc\n",
            " ECG_TransferLearningPaper.pdf\t Scheduling.gdoc\n",
            " ExampleNotebook.ipynb\t\t'Spring24: Project Grading rubrik.gdoc'\n",
            " jobs\t\t\t\t Tedgitrepo\n",
            "'Meeting Notes.gdoc'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ls $JOB_DIR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKvv_wWr96xy",
        "outputId": "0d99e612-a435-4736-89a1-bb5d0757fbcd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finetune_random_cnn_original_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Revision**\n",
        "\n",
        "We just create a new job subdirectory for this notebook.\n",
        "\n",
        "> `jobs/finetune_random_cnn_original_data_with_f1`\n",
        "\n",
        "---\n",
        "Note that while we did create the `jobs/` directory, we will defer the creation of fine tuning train specific output directory to the trainer code.\n",
        "\n",
        "So for this experiment, we will write the fine tune results out to `jobs/finetune_random_cnn_original_data`. The name indicates 2 things\n",
        "\n",
        "1. Random pretrained CNN used\n",
        "2. We use the same preprocessing steps the authors suggest in their README. Which again is **not** aligned with what they say in the paper."
      ],
      "metadata": {
        "id": "ChKcV_grjkNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other discrepancies\n",
        "\n",
        "This is the exact code that the authors say we should run finetuning with:\n",
        "\n",
        "```shell script\n",
        "python -m finetuning.trainer \\\n",
        "--job-dir \"jobs/af_classification\" \\\n",
        "--train \"data/physionet_train.pkl\" \\\n",
        "--test \"data/physionet_test.pkl\" \\\n",
        "--weights-file \"jobs/beat_classification/resnet18.weights\" \\\n",
        "--val-size 0.0625 \\\n",
        "--arch \"resnet18\" \\\n",
        "--batch-size 64 \\\n",
        "--epochs 200\n",
        "```\n",
        "\n",
        "The discrepancies:\n",
        "\n",
        "1. `--val-metric` is NOT specified. The default value is `loss`. The help message describes this parameter as\n",
        "\n",
        "  > Validation metric used to find the best model at each epoch.\n",
        "\n",
        "  However, in the paper, the authors say that they use macro F1 score to evaluate on validation set and also to select the best model.\n",
        "\n",
        "## Reproducibility\n",
        "\n",
        "The trainer code also has a `--seed` parameter, which is not provided in the above code snippet.\n",
        "\n",
        "For our own benefit, we shall set `--seed 2024` for reproducibility in *our* own work."
      ],
      "metadata": {
        "id": "Og6V6_NXkJ_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "job_dir = JOB_DIR + '/finetune_random_cnn_original_data_with_f1'\n",
        "train = DATA_DIR + '/physionet_finetune/physionet_train.pkl'\n",
        "test = DATA_DIR + '/physionet_finetune/physionet_test.pkl'\n",
        "\n",
        "print(f\"job_dir: {job_dir}\")\n",
        "print(f\"train: {train}\")\n",
        "print(f\"test: {test}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96dJL5rglyVq",
        "outputId": "9712782a-eeaa-493d-f487-205655eff2d8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "job_dir: /content/drive/MyDrive/DLHProject/jobs/finetune_random_cnn_original_data_with_f1\n",
            "train: /content/drive/MyDrive/DLHProject/data/physionet_finetune/physionet_train.pkl\n",
            "test: /content/drive/MyDrive/DLHProject/data/physionet_finetune/physionet_test.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we use teh same settings Myles ran when he tried the precursor notebook. Namely:\n",
        "\n",
        "1. V100 GPU (16 gb)\n",
        "2. batch 128\n"
      ],
      "metadata": {
        "id": "MEwT2uM9-ER6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# We've removed --weights-file parameter.\n",
        "# We've set --val-metric to f1\n",
        "# We've set --seed to 2024\n",
        "# We've set --verbose to see what's going on\n",
        "! python -m finetuning.trainer \\\n",
        "--job-dir $job_dir \\\n",
        "--train $train \\\n",
        "--test $test \\\n",
        "--val-size 0.0625 \\\n",
        "--val-metric \"f1\" \\\n",
        "--arch \"resnet18\" \\\n",
        "--batch-size 128 \\\n",
        "--epochs 200 \\\n",
        "--seed 2024 \\\n",
        "--verbose"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQSa8mSljD39",
        "outputId": "c74766f9-cba7-45b9-bf2d-ea086bc4a062"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-10 07:50:46.753615: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-10 07:50:46.753671: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-10 07:50:46.755060: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-10 07:50:46.762402: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-10 07:50:47.805824: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Creating working directory in /content/drive/MyDrive/DLHProject/jobs/finetune_random_cnn_original_data_with_f1\n",
            "Setting random state 2024\n",
            "Loading train data from /content/drive/MyDrive/DLHProject/data/physionet_finetune/physionet_train.pkl ...\n",
            "Split data into train 93.74% and validation 6.26%\n",
            "Loading test data from /content/drive/MyDrive/DLHProject/data/physionet_finetune/physionet_test.pkl ...\n",
            "Train data shape: (6395, 16384, 1)\n",
            "2024-04-10 07:51:11.115779: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-10 07:51:11.723859: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-10 07:51:11.724206: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-10 07:51:11.725244: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-10 07:51:11.725511: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-10 07:51:11.725704: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-10 07:51:11.827860: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-10 07:51:11.828249: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-10 07:51:11.828407: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2024-04-10 07:51:11.828513: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-10 07:51:11.828664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14792 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "2024-04-10 07:51:11.829626: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 838205440 exceeds 10% of free system memory.\n",
            "2024-04-10 07:51:12.677879: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 838205440 exceeds 10% of free system memory.\n",
            "Building model ...\n",
            "# model parameters: 4,494,532\n",
            "2024-04-10 07:51:15.454981: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 838205440 exceeds 10% of free system memory.\n",
            "2024-04-10 07:51:16.235770: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 838205440 exceeds 10% of free system memory.\n",
            "2024-04-10 07:51:17.036039: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 838205440 exceeds 10% of free system memory.\n",
            "Epoch 1/200\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/root/DLH_TransferLearning/finetuning/trainer.py\", line 184, in <module>\n",
            "    model.fit(train_data, epochs=args.epochs, verbose=2, validation_data=val_data, callbacks=callbacks)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/tmp/__autograph_generated_filewnchqvvf.py\", line 15, in tf__train_function\n",
            "    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n",
            "ValueError: in user code:\n",
            "\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n",
            "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n",
            "        outputs = model.train_step(data)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1155, in train_step\n",
            "        return self.compute_metrics(x, y, y_pred, sample_weight)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1249, in compute_metrics\n",
            "        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n",
            "        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n",
            "        result = update_state_fn(*args, **kwargs)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n",
            "        return ag_update_state(*args, **kwargs)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/metrics/f_score_metrics.py\", line 176, in update_state  **\n",
            "        y_true = tf.convert_to_tensor(y_true, dtype=self.dtype)\n",
            "\n",
            "    ValueError: Tensor conversion requested dtype float32 for Tensor with dtype uint8: <tf.Tensor 'cond/Identity_1:0' shape=(None, 4) dtype=uint8>\n",
            "\n",
            "CPU times: user 244 ms, sys: 38.2 ms, total: 282 ms\n",
            "Wall time: 37.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qd5EA-NFjD7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gBKO_kPbjD-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PDwcki_AjEAr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}