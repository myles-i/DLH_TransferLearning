{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune: figuring out why original code not recording f1 in history.csv\n",
        "\n",
        "Copy of `jupyter_notebooks/finetune_train_random_original.ipynb`,\n",
        "\n",
        "but now we try adding macro f1 metric to the model metrics to be evaluated on the validation set at epoch end. The goal is to get macro f1 scores into the output history.csv so that we can plot the validation score by epoch curve. This would be our attempt to reproduce Figure 3 in the paper.\n",
        "\n",
        "---\n",
        "\n",
        "Original prepared dataset is the code in `finetuning/readme.md` which samples at 250 hz, 65 seconds.\n",
        "\n",
        "Now we try training on an uninitialized CNN.\n",
        "\n",
        "We do this by *not* passing in the `--weights-file` parameter to `finetuning.trainer`."
      ],
      "metadata": {
        "id": "v1XRxBLDglRi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DNqG2WkWgjH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bfdd679-9e18-48d5-fd96-bc5bdf47cfa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# You may also manually mount drive by clicking on folder icon in left sidebar\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ROOT = '/content/drive/MyDrive/DLHProject'"
      ],
      "metadata": {
        "id": "M7jO-g3ziDbB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have colab pro, clone our repo to /root directory.\n",
        "\n",
        "Sample commands to run in Colab Pro Terminal:\n",
        "\n",
        "```bash\n",
        "$ cd /root\n",
        "# enter your username and github PAT\n",
        "$ git clone https://github.com/myles-i/DLH_TransferLearning.git\n",
        "$ cd DLH_TransferLearning\n",
        "```"
      ],
      "metadata": {
        "id": "UDppmL8e9XEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REPO = PROJECT_ROOT + '/Danielgitrepo'\n",
        "# Below is if you have colab pro\n",
        "REPO = '/root/DLH_TransferLearning/'"
      ],
      "metadata": {
        "id": "-zVHB1B1iDd3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd $REPO"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL9h8ntdiDhp",
        "outputId": "5352a47a-f362-4057-e6bf-e3c145fab506"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/DLH_TransferLearning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "hKEvhfLriULt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = PROJECT_ROOT + '/data'"
      ],
      "metadata": {
        "id": "J_SX1ghHiUH0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls $DATA_DIR"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXzD5t0riUEl",
        "outputId": "a41203b8-bae7-47c5-b073-997549c08c2a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "icentia11k\t\t     icentia11k_subset_unzipped\t\t physionet_finetune\n",
            "icentia11k_corrupted\t     physionet\t\t\t\t physionet_preread\n",
            "icentia11k_subset\t     physionet_250hz_15000pad_norm_True  session_checkpoint.dat\n",
            "icentia11k_subset_corrupted  physionet_data.zip\t\t\t temp.torrent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up main job dir that will hold the results of this (and other finetuning) job."
      ],
      "metadata": {
        "id": "oOPCMQFLilP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "JOB_DIR = PROJECT_ROOT + '/jobs'"
      ],
      "metadata": {
        "id": "dJN-qWu-iUBv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Revision**\n",
        "\n",
        "We just create a new job subdirectory for this notebook.\n",
        "\n",
        "> `jobs/finetune_random_cnn_original_data_with_f1`\n",
        "\n",
        "---\n",
        "Note that while we did create the `jobs/` directory, we will defer the creation of fine tuning train specific output directory to the trainer code.\n",
        "\n",
        "So for this experiment, we will write the fine tune results out to `jobs/finetune_random_cnn_original_data`. The name indicates 2 things\n",
        "\n",
        "1. Random pretrained CNN used\n",
        "2. We use the same preprocessing steps the authors suggest in their README. Which again is **not** aligned with what they say in the paper."
      ],
      "metadata": {
        "id": "ChKcV_grjkNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other discrepancies\n",
        "\n",
        "This is the exact code that the authors say we should run finetuning with:\n",
        "\n",
        "```shell script\n",
        "python -m finetuning.trainer \\\n",
        "--job-dir \"jobs/af_classification\" \\\n",
        "--train \"data/physionet_train.pkl\" \\\n",
        "--test \"data/physionet_test.pkl\" \\\n",
        "--weights-file \"jobs/beat_classification/resnet18.weights\" \\\n",
        "--val-size 0.0625 \\\n",
        "--arch \"resnet18\" \\\n",
        "--batch-size 64 \\\n",
        "--epochs 200\n",
        "```\n",
        "\n",
        "The discrepancies:\n",
        "\n",
        "1. `--val-metric` is NOT specified. The default value is `loss`. The help message describes this parameter as\n",
        "\n",
        "  > Validation metric used to find the best model at each epoch.\n",
        "\n",
        "  However, in the paper, the authors say that they use macro F1 score to evaluate on validation set and also to select the best model.\n",
        "\n",
        "## Reproducibility\n",
        "\n",
        "The trainer code also has a `--seed` parameter, which is not provided in the above code snippet.\n",
        "\n",
        "For our own benefit, we shall set `--seed 2024` for reproducibility in *our* own work."
      ],
      "metadata": {
        "id": "Og6V6_NXkJ_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "job_dir = JOB_DIR + '/finetune_random_cnn_original_data_with_f1'\n",
        "train = DATA_DIR + '/physionet_finetune/physionet_train.pkl'\n",
        "test = DATA_DIR + '/physionet_finetune/physionet_test.pkl'\n",
        "\n",
        "print(f\"job_dir: {job_dir}\")\n",
        "print(f\"train: {train}\")\n",
        "print(f\"test: {test}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96dJL5rglyVq",
        "outputId": "67df3ce8-9d75-49bf-dde6-023e34946d76"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "job_dir: /content/drive/MyDrive/DLHProject/jobs/finetune_random_cnn_original_data_with_f1\n",
            "train: /content/drive/MyDrive/DLHProject/data/physionet_finetune/physionet_train.pkl\n",
            "test: /content/drive/MyDrive/DLHProject/data/physionet_finetune/physionet_test.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we use the same settings Myles ran when he tried the precursor notebook. Namely:\n",
        "\n",
        "1. V100 GPU (16 gb)\n",
        "2. batch 128\n",
        "\n",
        "---\n",
        "\n",
        "**NEW** for this pass, we keep `--epochs=10` initially to see if my changes are working. Namely do we get macro f1 score?\n"
      ],
      "metadata": {
        "id": "MEwT2uM9-ER6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# We've removed --weights-file parameter.\n",
        "# We've set --val-metric to f1\n",
        "# We've set --seed to 2024\n",
        "# We've set --verbose to see what's going on\n",
        "! python -m finetuning.trainer \\\n",
        "--job-dir $job_dir \\\n",
        "--train $train \\\n",
        "--test $test \\\n",
        "--val-size 0.0625 \\\n",
        "--val-metric \"f1\" \\\n",
        "--arch \"resnet18\" \\\n",
        "--batch-size 128 \\\n",
        "--epochs 10 \\\n",
        "--seed 2024 \\\n",
        "--verbose"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQSa8mSljD39",
        "outputId": "8a21333a-a14b-4cbd-f862-0f49bdb7ef99"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-11 06:20:06.506652: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-11 06:20:06.506746: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-11 06:20:06.508684: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-11 06:20:06.519572: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-11 06:20:08.019033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Creating working directory in /content/drive/MyDrive/DLHProject/jobs/finetune_random_cnn_original_data_with_f1\n",
            "Setting random state 2024\n",
            "Loading train data from /content/drive/MyDrive/DLHProject/data/physionet_finetune/physionet_train.pkl ...\n",
            "Split data into train 93.74% and validation 6.26%\n",
            "Loading test data from /content/drive/MyDrive/DLHProject/data/physionet_finetune/physionet_test.pkl ...\n",
            "Train data shape: (6395, 16384, 1)\n",
            "2024-04-11 06:20:28.996528: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-11 06:20:29.553376: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-11 06:20:29.553678: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-11 06:20:29.554708: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-11 06:20:29.555033: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-11 06:20:29.555272: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-11 06:20:29.652785: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-11 06:20:29.653143: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-11 06:20:29.653280: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2024-04-11 06:20:29.653370: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-11 06:20:29.653507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14792 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n",
            "2024-04-11 06:20:29.654455: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 838205440 exceeds 10% of free system memory.\n",
            "2024-04-11 06:20:30.498149: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 838205440 exceeds 10% of free system memory.\n",
            "Building model ...\n",
            "# model parameters: 4,494,532\n",
            "2024-04-11 06:20:33.283049: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 838205440 exceeds 10% of free system memory.\n",
            "2024-04-11 06:20:34.062739: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 838205440 exceeds 10% of free system memory.\n",
            "2024-04-11 06:20:34.851692: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 838205440 exceeds 10% of free system memory.\n",
            "Epoch 1/10\n",
            "2024-04-11 06:20:43.655374: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8906\n",
            "2024-04-11 06:20:50.046785: I external/local_xla/xla/service/service.cc:168] XLA service 0x7e6a758f41c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2024-04-11 06:20:50.046832: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
            "2024-04-11 06:20:50.067688: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1712816450.200845    2825 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00001: f1 improved from -inf to 0.40086, saving model to /content/drive/MyDrive/DLHProject/jobs/finetune_random_cnn_original_data_with_f1/best_model.weights\n",
            "50/50 - 47s - loss: 0.9433 - acc: 0.6013 - val_loss: 0.9501 - val_acc: 0.6089 - f1: 0.4009 - 47s/epoch - 946ms/step\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 0s 98ms/step\n",
            "\n",
            "Epoch 00002: f1 improved from 0.40086 to 0.61453, saving model to /content/drive/MyDrive/DLHProject/jobs/finetune_random_cnn_original_data_with_f1/best_model.weights\n",
            "50/50 - 22s - loss: 0.7666 - acc: 0.6783 - val_loss: 0.6366 - val_acc: 0.7213 - f1: 0.6145 - 22s/epoch - 435ms/step\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 1s 104ms/step\n",
            "\n",
            "Epoch 00003: f1 (0.41772) did not improve from 0.61453\n",
            "50/50 - 22s - loss: 0.6997 - acc: 0.7134 - val_loss: 0.8630 - val_acc: 0.6487 - f1: 0.4177 - 22s/epoch - 438ms/step\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 0s 98ms/step\n",
            "\n",
            "Epoch 00004: f1 (0.59073) did not improve from 0.61453\n",
            "50/50 - 21s - loss: 0.6476 - acc: 0.7292 - val_loss: 0.7143 - val_acc: 0.7307 - f1: 0.5907 - 21s/epoch - 423ms/step\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 0s 99ms/step\n",
            "\n",
            "Epoch 00005: f1 improved from 0.61453 to 0.66907, saving model to /content/drive/MyDrive/DLHProject/jobs/finetune_random_cnn_original_data_with_f1/best_model.weights\n",
            "50/50 - 22s - loss: 0.6315 - acc: 0.7354 - val_loss: 0.6611 - val_acc: 0.7494 - f1: 0.6691 - 22s/epoch - 435ms/step\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00006: f1 (0.60157) did not improve from 0.66907\n",
            "50/50 - 21s - loss: 0.6043 - acc: 0.7561 - val_loss: 0.7462 - val_acc: 0.7377 - f1: 0.6016 - 21s/epoch - 427ms/step\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00007: f1 (0.34476) did not improve from 0.66907\n",
            "50/50 - 21s - loss: 0.5666 - acc: 0.7694 - val_loss: 1.9298 - val_acc: 0.3536 - f1: 0.3448 - 21s/epoch - 426ms/step\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00008: f1 improved from 0.66907 to 0.71554, saving model to /content/drive/MyDrive/DLHProject/jobs/finetune_random_cnn_original_data_with_f1/best_model.weights\n",
            "50/50 - 22s - loss: 0.5416 - acc: 0.7842 - val_loss: 0.5813 - val_acc: 0.7939 - f1: 0.7155 - 22s/epoch - 440ms/step\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 1s 99ms/step\n",
            "\n",
            "Epoch 00009: f1 (0.66142) did not improve from 0.71554\n",
            "50/50 - 21s - loss: 0.5337 - acc: 0.7817 - val_loss: 0.6624 - val_acc: 0.7611 - f1: 0.6614 - 21s/epoch - 429ms/step\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 1s 100ms/step\n",
            "\n",
            "Epoch 00010: f1 improved from 0.71554 to 0.72754, saving model to /content/drive/MyDrive/DLHProject/jobs/finetune_random_cnn_original_data_with_f1/best_model.weights\n",
            "50/50 - 22s - loss: 0.5219 - acc: 0.7891 - val_loss: 0.6001 - val_acc: 0.7892 - f1: 0.7275 - 22s/epoch - 438ms/step\n",
            "Loading the best weights from file /content/drive/MyDrive/DLHProject/jobs/finetune_random_cnn_original_data_with_f1/best_model.weights ...\n",
            "Predicting training data ...\n",
            "50/50 [==============================] - 6s 104ms/step\n",
            "Predicting validation data ...\n",
            "4/4 [==============================] - 0s 99ms/step\n",
            "Predicting test data ...\n",
            "14/14 [==============================] - 2s 141ms/step\n",
            "CPU times: user 1.95 s, sys: 220 ms, total: 2.17 s\n",
            "Wall time: 5min 14s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It took 5m 14s to run 10 epochs"
      ],
      "metadata": {
        "id": "QdSDq1xU1UHz"
      }
    }
  ]
}