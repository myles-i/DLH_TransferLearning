{"cells":[{"cell_type":"markdown","metadata":{"id":"ypFTXiFx6OIM"},"source":["# Basic Info\n","\n","## Team 1\n","- Ted Hsu (thhsu4@illinois.edu)\n","- Myles Iribarne (mylesai2@illinois.edu)\n","- Daniel Xu (dhxu2@illinois.edu)\n","\n","## Paper\n","\n","Our paper is _Transfer learning for ECG classification_ by Weimann and Conrad [1]. The project code is available on Github [2].\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MQ0sNuMePBXx"},"source":["# Introduction\n","## Background of the problem\n","\n","- **What type of problem**:\n","\n","  The problem is to classify Atrial Fibrillation (AF) on electrocardiogram (ECG) recordings.\n","- **What is the importance/meaning of solving the problem**:\n","\n","  - A solution to the problem is a tool that will assist physicians in analyzing large amounts of patient ECG data in an automated and time efficient manner.\n","  - Early detection of AF events may lead to better patient outcomes.\n","  \n","- **What is the difficulty of the problem**:\n","\n","  - Devices for recording patient ECG data are able to output a _huge_ amount of raw data. This is challenging and expensive to annotate for effective Deep Learning training.\n","  - Large class imbalance due to cardiovascular events of interests being rare.\n","  - Low ECG signal quality due to sampling frequency, single ECG lead probe.\n","\n","- **The state of the art methods and effectiveness**:\n","\n","  - Transfer learning using 1-D residual networks [3]\n","  - Representation learning using encoder-decoder architectures\n","    - Stacked Denoising AEs [4]\n","    - Seq2Seq model [5]\n","\n","## Paper Explanation\n","- **What did the paper propose**:\n","\n","  - Use Transfer learning to build better ECG classifiers.\n","  - Pre-train 1-D CNNs on the largest publicly available ECG dataset (_Icentia11K_) on several pre-training tasks:\n","    - Beat Classification\n","    - Rhythm Classifcation\n","    - Heart Rate Classification\n","    - Future Prediction\n","  - Finetune the pre-trained 1-D CNNs on a _different_ task and a _different_ dataset (_PhysioNet/CinC Challenge 2017_): classify AF events.\n","\n","- **What is/are the innovations of the method**:\n","\n","  - Demonstration of successful large-scale pre-training of 1-D CNNs on the largest publicly available ECG dataset to date.\n","  - Demonstration of contrastive pre-training (unsupervised representation learning) improving 1-D CNN performance on target task.\n","  - Novel usage of heart rate classification task for pre-training. Note that in this task, the labels can be automatically generated without manual intervention.\n","\n","- **How well the proposed method work (in its own metrics)**:\n","\n","  - The paper provides AF classifier performnace comparison among five different pre-training tasks configurations (Random initalization, Beat classification, Rhythm classification, Heart Rate classification, and Future Predicition). Average macro F1 score of the AF classifier on the Physionet test set is the performance metric.\n","  - The macro F1 score of random initalization pre-training task is 0.731. F1 scores reported by all proposed four pre-training tasks configurations range from 0.758 to 0.779.\n","\n","- **What is the contribution to the research regime (referring the Background above, how important the paper is to the problem)**:\n","\n","  - Pre-training the 1-D CNN model improves the performance on the target task (i.e. AF classification), effectively reducing the number of labeled data required to achieve the same performance as 1-D CNNs that are not pre-trained.\n","  - Unsupervised pre-training (i.e. future prediction) on ECG data is a viable method for improving the performance on the target task and will become more relevant, since labeling ECG data is expensive."]},{"cell_type":"markdown","metadata":{"id":"uygL9tTPSVHB"},"source":["# Scope of Reproducibility\n","\n","## Hypothesis 1\n","Pre-training 1-D CNN models with an extremely large dataset of relatively inexpensively labeled data can improve performance of classification based on a smaller set of labeled data with a different classification objective (i.e. AF).\n","\n","## Hypothesis 2\n","The paper does not explore how significant the effects of the pre-training data size are on the final results. We assume size of the pre-training dataset could affect the performance of the target task (i.e. AF classification).\n","\n","\n","## Verification\n","We will verify the hypotheses by attempting to reproduce results for a specific model and the following hyperparameter combination with 10% and 20% of the pre-training data used in the paper:\n","\n","- Model: 1-D ResNet-18v2\n","- Pre-training Objective: Beat Classification\n","- Frame Size: 4096 (samples)\n","- Sample Rate: 250 Hz\n","- Fine-tuning objective: Atrial Fibrillation\n","\n","The results will be compared with the performance of a randomly initialized ResNetv2.\n","\n","# Ablation (Hypothesis 3)\n"," The original paper is entirely based on 1-D CNNs and the raw ECG signal. To extend the paper's results, we aim to pre-process the raw signals using Fourier transforms to represent the data as a spectogram -- a frequency versus time representation of ECG signals. Using this representation of the input, we will train a 2-D CNN model (i.e. 2-D ResNetv18) and compare the performance of pre-trained and randomly initialized models. Additionally, we will compare the 2-D model performance to the 1-D models originally used by the authors.\n","\n"," This extension is motivated by a study on ECG Arrhythmia classification that demonstrates the effectiveness of CNNs trained on spectrograms.[6] By converting ECG data to spectrogram features and then using spectrograms to pre-train a 2-D ResNet, we intend to illustrate the adaptability of the transfer learning framework in the original paper across diverse model architectures.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xWAHJ_1CdtaA"},"source":["# Methodology\n","\n","## Environment\n","All of the project's codes, data, and files are in a shared Google drive. Users of this notebook are required to mount the shared drive in a Google Colab notebook.\n","\n","1. Create a shortcut to the shared Google drive from your own Google Drive.\n","2. Modify the `PROJECT_ROOT` variable below accordingly.\n","\n","The link to the project's shared Google drive: https://drive.google.com/drive/folders/1vlUILM7cToH5CoX1x0kWRpe55MbBogS-?usp=drive_link"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jk2v_aWOfdRv"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ONI_oHIHu9Zz"},"outputs":[],"source":["# modify based on shortcut to shared Google drive\n","PROJECT_ROOT = '/content/drive/MyDrive/UIUC MCS/CS598 Deep Learning for Healthcare/Project'\n","DATA_DIR = PROJECT_ROOT + '/data'"]},{"cell_type":"markdown","metadata":{"id":"8_d-BnwhwM60"},"source":["Install required packages. Note that the `%%capture` cell magic is used to suppress the output of `pip install` for brevity."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xZWw8dgEtREL"},"outputs":[],"source":["%%capture\n","%cd $PROJECT_ROOT\n","! pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"2NbPHUTMbkD3"},"source":["##  Data\n","\n","### Pre-training Dataset\n"," The training data is the “Icentia11k Single Lead Continuous Raw Electrocardiogram Dataset,” which is freely available online.\n"," * Source of the data\n","  * https://physionet.org/content/icentia11k-continuous-ecg/1.0/ (raw)\n","  * https://academictorrents.com/details/af04abfe9a3c96b30e5dd029eb185e19a7055272 (compressed)\n"," * Statistics\n","  * 11,000 patients\n","  * Each patient has upto two weeks of ECG recordings with 250Hz sampling rate.\n","  * Each ECG recording is acompanied with beat and rhythm labels marked by the ECG signal collection device and specialists, respectively.\n","  * Both beat and rhythm labels are assigned to positions in the signal at irregular intervals.\n","  * The original paper uses 95% of the patients for pre-training and the remaining 5% for validation.\n"," * Data downlaoding:\n","   * See [this notebook ](https://github.com/myles-i/DLH_TransferLearning/blob/master/jupyter_notebooks/Download_Icentia11k_Data.ipynb) our group wrote which uses the python libtorrent library to download the compressed version of the data from academictorrents.com\n","\n","\n","### Fine-tuning Dataset\n","The fine-tuning dataset is the “AF Classification from a Short Single Lead ECG Recording: The PhysioNet/Computing in Cardiology Challenge 2017” and freely avaiable online for download.\n"," * Source of the data\n","  * https://physionet.org/content/challenge-2017/1.0.0/\n"," * Statistics\n","  * 8528 short ECG recordings\n","  * Each ECG recording duration is 9 to 60 seconds with 300Hz sampling rate\n","  * Each ECG recording is labeled with one of the following classes: AF, Normal, Other or Noise (too noisy to classify).\n"," * Data process (TODO: add more detail)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3muyDPFPbozY"},"source":["##   Model used by author\n","In this project, CNN model of the choice is ResNet-18. 1D ResNet-18 implemented in the paper's github [2] is used to reproduce the paper's result, and 2D ResNet-18 for abalation task.\n","\n","### 1D ResNet-18\n","  * Model architecture\n","    * 18 layers\n","    * Input layer consists of convolution layer with 64 filters, kernel size=3 and stride=2. The output of the convolution layer passes through batch norm, ReLu and maxpooling layer sequentially.\n","    * Output layer is a clssifier consisting of densely-connected layer followed by softmax function.\n","    * The middle 16 layers consists of 8 residual blocks. A residual block consists of the following two components and outputs the sum of the two compoenets' outputs.\n","      1. two convolution layers, each followed by batch norm and ReLu\n","      2. a shortcut that passes the input through a convolution layer followed by batch norm.\n","    * Configurations of the residual blocks\n","      * 1st and 2nd: 64 filters, kernel size=7, strides=2 and 1, respectively\n","      * 3rd and 4th: 128 filters, kernel size=5, strides=2 and 1, respectively\n","      * 5th and 6th: 256 filters, kernel size=5, strides=2 and 1, respectively\n","      * 7th and 8th: 512 filters, kernel size=3, strides=2 and 1, respectively\n","    * Detail: https://github.com/myles-i/DLH_TransferLearning/blob/master/transplant/modules/resnet1d.py\n","\n","  * Training objectives\n","    * loss function: `tf.keras.optimizers.Adam(beta_1=0.9, beta_2=0.98, epsilon=1e-9)`\n","    * optimizer: `tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)`\n","    * metric: `tf.keras.metrics.SparseCategoricalAccuracy(name='acc')`\n"]},{"cell_type":"markdown","metadata":{"id":"PjhVgufs6OIR"},"source":["## Training\n","\n","### Computational Requirements\n","\n","We use Google Colab Pro, V100 GPUs.\n","\n","### Implementation code\n","\n","For both pre-training and fine tuning, the paper provides API to run the entire process with parameters of choice. Following is the high level description of how the API works.\n","\n","#### Pre-training\n","  1. Create train/validate data generator based on patient id and the number of samples per patient, both specified when calling the API.\n","  2. A model is generated based on the model architectrue and pre-training task specified by the API user.\n","  3. Weights of the model are initialized. They can also be loaded from a weights file. For all pre-training in the project, we don't load weights.\n","  4. Check point function is created based on training metric. For pre-training, we use `loss` as training metric.\n","  5. The model fits the train data. At the end of each training epoch, the check point function is called for evaluation and save the model weights.\n","\n","#### Fine tuning\n","  1. Train and test data set are already separated from PhysioNet 2017 dataset, with a 80%/20% split, and are passed to the API. The validation dataset will be further sperated from the train dataset based on user input.\n","  2. A CNN model is generated based on the model architectrue specified by the API user. A binary classifier is attached to the output of the CNN model as output layer.\n","  3. Weights of the model are initialized. They can also be loaded from a weights file.\n","  4. Check point function is created based on training metric. For pre-training, we use `f1` as training metric.\n","  5. The model fits the train data. At the end of each training epoch, the check point function is called for evaluation and save the model weights.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"40VeOBV16OIR"},"source":["## Evaluation\n","\n","### Metrics Descriptions\n","\n","F1 metric\n","\n","### Implementation code\n","\n","####Pre-training\n","The paper uses 95% of the patient's ECG data. On average, the paper sample 4096 ECG frames per patient, which amounts to 42.8 million (11000x0.95x4096) training samples over the course of pretraining. For pre-training with 20% of the data used in paper, we use ECG data from 2048 patients and sample 4096 ECG frames per patient, resulting to roughl 8.4 million (2048x4096) training samples."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W9TLC4hj2M72"},"outputs":[],"source":["TRAIN_DATASET = DATA_DIR + '/icentia11k'"]},{"cell_type":"markdown","metadata":{"id":"wnsQfTsc26yA"},"source":["Following uses the API to run pre-training with 20% of the data used in the paper. Uncommet the cell to run.\n","\n","* `--job-dir`: output directory, where check points and weights are saved\n","\n","* `--task`: pre-training task, \"beat\" for Beat classification\n","\n","* `--train`: training dataset directory\n","\n","* `--arch`: CNN architecture\n","\n","* `--patient-ids`: patient id whose ECG data to be used in pre-training\n","\n","* `--frame-size`: number of ECG samples, with 250Hz sampling rate, in a ECG frame\n","\n","* To use all data: number of patients x samples_per_patient = epochs x batch_size x steps-per-epoch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBVu9Fas18M4"},"outputs":[],"source":["# !time python -m pretraining.trainer \\\n","# --job-dir \"jobs/beat_classification_16epochs_to_20percent_round3\" \\\n","# --task \"beat\" \\\n","# --train $TRAIN_DATASET \\\n","# --arch \"resnet18\" \\\n","# --epochs  16\\\n","# --patient-ids `seq 0 2047 | paste -sd, -` \\\n","# --steps-per-epoch 1024 \\\n","# --samples-per-patient 4096 \\\n","# --batch-size 512 \\\n","# --frame-size 4096"]},{"cell_type":"markdown","metadata":{"id":"tAzi8Aa06Zh2"},"source":["####Fine Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d2TJ3mzh6wUB"},"outputs":[],"source":["JOB_DIR = PROJECT_ROOT + '/jobs/fine_tune_random_resnet18'\n","FINETUNE_TRAIN = DATA_DIR + '/physionet_finetune/physionet_train.pkl'\n","FINETUNE_TEST = DATA_DIR + '/physionet_finetune/physionet_test.pkl'"]},{"cell_type":"markdown","metadata":{"id":"Yd-4RslV7b0o"},"source":["Following uses the API to run fine tuning with random initialization. Uncomment to run.\n","* `--weights-file $WEIGHTS_FILE`: Path to pretrained weights or a checkpoint of the model to be used for model initialization. Random initilization if not specified.\n","\n","* `--val-size 0.0625`: This is the percentage of the train set size to set aside for the validation set. Note that the PhysioNet data was already split 80-20 train-test. The paper uses 5 percent of the full dataset for validation. We get this via 0.0625x0.8=0.05\n","\n","* `--val-metric \"f1\"`: Use macro F1 score to evaluate performance on validation set and to find the best model at each epoch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sV1EBd7Z6nr_"},"outputs":[],"source":["# %%time\n","# !python -m finetuning.trainer \\\n","# --job-dir $JOB_DIR \\\n","# --train $FINETUNE_TRAIN \\\n","# --test $FINETUNE_TEST \\\n","# --val-size 0.0625 \\\n","# --val-metric \"f1\" \\\n","# --arch \"resnet18\" \\\n","# --batch-size 128 \\\n","# --epochs 200 \\\n","# --seed 2024 \\\n","# --verbose"]},{"cell_type":"markdown","metadata":{"id":"yNnSDq6w7mDX"},"source":["Following uses the API to run fine tuning with pre-traning weights. Uncomment to run."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vb51wGvO8Cac"},"outputs":[],"source":["JOB_DIR = PROJECT_ROOT + '/jobs/fine_tune_pre_trained_20_resnet18'\n","WEIGHTS_FILE = PROJECT_ROOT + '/jobs/beat_classification/pre_trained_20_resnet18.weights'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wVpmhIw_7tpO"},"outputs":[],"source":["# %%time\n","# !python -m finetuning.trainer \\\n","# --job-dir $JOB_DIR \\\n","# --train $FINETUNE_TRAIN \\\n","# --test $FINETUNE_TEST \\\n","# --weights-file $WEIGHTS_FILE \\\n","# --val-size 0.0625 \\\n","# --val-metric \"f1\" \\\n","# --arch \"resnet18\" \\\n","# --batch-size 128 \\\n","# --epochs 200 \\\n","# --seed 2024 \\\n","# --verbose"]},{"cell_type":"markdown","metadata":{"id":"IBV2Q_yrsppa"},"source":[]},{"cell_type":"markdown","metadata":{"id":"gX6bCcZNuxmz"},"source":["# Reproduction Results\n","We compare the validation F1 between model with random initialization weights and pre-training weights.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LjW9bCkouv8O"},"outputs":[],"source":["import pandas as pd\n","from matplotlib import pyplot as plt\n","\n","\n","random_result = pd.read_csv(PROJECT_ROOT + '/jobs/finetune_baseline_65sec/history.csv')\n","pretrain_20_result = pd.read_csv(PROJECT_ROOT + '/jobs/finetune_pretrain_20_weights_65sec/history.csv')\n","\n","plt.plot(random_result['epoch'], random_result['f1'], color='tab:red', label='random')\n","plt.plot(pretrain_20_result['epoch'], pretrain_20_result['f1'], color='blue', label='pretrain 20')\n","plt.xlabel('Epoch')\n","plt.ylabel('Validation F1')\n","ax = plt.gca()\n","ax.set_xlim([0, 80])\n","ax.set_ylim([0.5, 1.0])\n","plt.legend(loc='upper right')"]},{"cell_type":"markdown","metadata":{"id":"o-7ii-4FA6Og"},"source":["## Reproduction Analysis\n","With pre-training weights, the F1 scores stablize much sooner than random initialization. Model with pre-training weights also outperform random initalization model in the most part.\n","\n","\n","\n","## Plan\n","The paper runs fine tuning 10 times for each weights initilization method and then plot the F1 versus Epoch graph. We only run fine tuning once for each  weights initilization method. We can run fine tuning more times and then compare with Figure 3 (a) in the paper again."]},{"cell_type":"markdown","metadata":{"id":"cxA8ecxzthX5"},"source":["# Ablation Study (Hypothesis 3)\n","For the ablation study, we are pre-processing the data to produce a spectogram for each ecg signal, and passing this result into a 2D ResNet. Then, the same pre-traning and finetuning process from the paper is followed. The goal is to test how extensible the papers ideas are to other models. The work-in-progress notebook that includes building the model in keras, the data pre-processing function, the keras data generators, and running two epochs of pre-training. Fine-tuning is not yet complete:\n","\n","https://github.com/myles-i/DLH_TransferLearning/blob/master/jupyter_notebooks/explore_spectogram.ipynb\n","\n","Below, we explain the preprocessing and model in more detail\n","\n","### Ablation Preprocessing - spectogram\n","For the spectogram, we chose the following parameters were chosen:\n","- window_size: 256 (~1 second)\n","- stride: 64 (~0.25 seconds)\n","- window type: hanning -> this is like a pre-defined convolution that isused to smooth the FFTs for each spectogram slice\n","\n","After the spectogram for a data sample is computed, only the lower 32 frequency components (from 0-62.5Hz) are selected to reduce the input size. This was chosen empirically and validated by reproducing the original signal with the filtered spectogram models.\n","\n","Below shows an example input sample, and its spectogram:\n","\n","![sample_signal](https://github.com/myles-i/DLH_TransferLearning/blob/master/report/images/sample_signal.PNG?raw=1)\n","\n","![sample_spectogram](https://github.com/myles-i/DLH_TransferLearning/blob/master/report/images/sample_spectogram.PNG?raw=1)\n","\n","\n","### Ablation Model: 2D ResNet-18\n","The model chose for the ablation study using spectograms is similar to the original model used, but is a 2D ResNet. It is presented here:\n","  * Model architecture\n","    * 18 layers\n","    * Input layer consists of convolution layer with 64 filters, kernel size=7x7 and stride=2. The output of the convolution layer passes through batch norm, ReLu and maxpooling layer sequentially.\n","\n","    * The middle 16 layers consists of 8 residual blocks. A residual block consists of the following two components and outputs the sum of the two compoenets' outputs.\n","      1. two convolution layers, each followed by batch norm and ReLu\n","      2. a shortcut that passes the input through a convolution layer followed by batch norm.\n","    * Output layer is a clssifier consisting of densely-connected layer followed by softmax or sigmoid function.\n","    * Configurations of the residual blocks\n","      * 1st and 2nd: 64 filters, kernel size=3x3, strides=2 and 1, respectively\n","      * 3rd and 4th: 128 filters, kernel size=3x3, strides=2 and 1, respectively\n","      * 5th and 6th: 256 filters, kernel size=3x3, strides=2 and 1, respectively\n","      * 7th and 8th: 512 filters, kernel size=3x3, strides=2 and 1, respectively\n","\n","  * Training objectives\n","    * loss function: `tf.keras.optimizers.Adam(beta_1=0.9, beta_2=0.98, epsilon=1e-9)`\n","    * optimizer: `tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)`\n","    * metric: `tf.keras.metrics.SparseCategoricalAccuracy(name='acc')`\n","\n","\n"]}],"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"10CnAxawFisX2w0kj3byNjp5CEMio8aqr","timestamp":1712988627392},{"file_id":"1MGxB_J2TvhAANcQG8VNMvQp1QdQrcxWb","timestamp":1709325387341},{"file_id":"1oAKqszNlwEZwPa_BjHPqfcoWlikYBpi5","timestamp":1709153069464}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
