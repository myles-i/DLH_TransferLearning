{"cells":[{"cell_type":"markdown","metadata":{"id":"ypFTXiFx6OIM"},"source":["# Basic Info\n","\n","## Team 1\n","- Ted Hsu (thhsu4@illinois.edu)\n","- Myles Iribarne (mylesai2@illinois.edu)\n","- Daniel Xu (dhxu2@illinois.edu)\n","\n","## Paper\n","\n","Our paper is _Transfer learning for ECG classification_ by Weimann and Conrad [1]. The project code is available on Github [2].\n","\n","## Repo\n","\n","The repo with our code is available on [Github](https://github.com/myles-i/DLH_TransferLearning/tree/master). Specifically, the reports are located [here](https://github.com/myles-i/DLH_TransferLearning/tree/master/report).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MQ0sNuMePBXx"},"source":["# Introduction\n","## Background of the problem\n","\n","- **What type of problem**:\n","\n","  The problem is to classify Atrial Fibrillation (AF) on electrocardiogram (ECG) recordings.\n","- **What is the importance/meaning of solving the problem**:\n","\n","  - A solution to the problem is a tool that will assist physicians in analyzing large amounts of patient ECG data in an automated and time efficient manner.\n","  - Early detection of AF events may lead to better patient outcomes.\n","  \n","- **What is the difficulty of the problem**:\n","\n","  - Devices for recording patient ECG data are able to output a _huge_ amount of raw data. This is challenging and expensive to annotate for effective Deep Learning training.\n","  - Large class imbalance due to cardiovascular events of interests being rare.\n","  - Low ECG signal quality due to sampling frequency, single ECG lead probe.\n","\n","- **The state of the art methods**:\n","\n","  - Transfer learning using 1-D residual networks [3]\n","  - Representation learning using encoder-decoder architectures\n","    - Stacked Denoising AEs [4]\n","    - Seq2Seq model [5]\n","\n","## Paper Explanation\n","- **What did the paper propose**:\n","\n","  - Use Transfer learning to build better ECG classifiers.\n","  - Pre-train 1-D CNNs on the largest publicly available ECG dataset (_Icentia11K_) on several pre-training tasks:\n","    - Beat Classification\n","    - Rhythm Classifcation\n","    - Heart Rate Classification\n","    - Future Prediction\n","  - Finetune the pre-trained 1-D CNNs on a _different_ task and a _different_ dataset (_PhysioNet/CinC Challenge 2017_): classify AF events.\n","\n","- **What is/are the innovations of the method**:\n","\n","  - Demonstration of successful large-scale pre-training of 1-D CNNs on the largest publicly available ECG dataset to date.\n","  - Demonstration of contrastive pre-training (unsupervised representation learning) improving 1-D CNN performance on target task.\n","  - Novel usage of heart rate classification task for pre-training. Note that in this task, the labels can be automatically generated without manual intervention.\n","\n","- **How well the proposed method work (in its own metrics)**:\n","\n","  - The paper provides AF classifier performnace comparison among five different pre-training tasks configurations (Random initalization, Beat classification, Rhythm classification, Heart Rate classification, and Future Predicition). Average macro F1 score of the AF classifier on the Physionet test set is the performance metric.\n","  - The macro F1 score of random initalization pre-training task is 0.731. F1 scores reported by all proposed four pre-training tasks configurations range from 0.758 to 0.779.\n","\n","- **What is the contribution to the research regime (referring the Background above, how important the paper is to the problem)**:\n","\n","  - Pre-training the 1-D CNN model improves the performance on the target task (i.e. AF classification), effectively reducing the number of labeled data required to achieve the same performance as 1-D CNNs that are not pre-trained.\n","  - Unsupervised pre-training (i.e. future prediction) on ECG data is a viable method for improving the performance on the target task and will become more relevant, since labeling ECG data is expensive."]},{"cell_type":"markdown","metadata":{"id":"uygL9tTPSVHB"},"source":["# Scope of Reproducibility\n","\n","## Hypothesis 1\n","Pre-training 1-D CNN models with an extremely large dataset of relatively inexpensively labeled data can improve performance of classification based on a smaller set of labeled data with a different classification objective (i.e. AF).\n","\n","## Hypothesis 2\n","The paper does not explore how significant the effects of the pre-training data size are on the final results. We assume size of the pre-training dataset could affect the performance of the target task (i.e. AF classification).\n","\n","\n","## Verification\n","We will verify the hypotheses by attempting to reproduce results for a specific model and the following hyperparameter combination with 10% and 20% of the pre-training data used in the paper:\n","\n","- Model: 1-D ResNet-18v2\n","- Pre-training Objective: Beat Classification\n","- Frame Size: 4096 (samples)\n","- Sample Rate: 250 Hz\n","- Fine-tuning objective: Atrial Fibrillation\n","\n","The results will be compared with the performance of a randomly initialized ResNetv2.\n","\n","# Ablation (Hypothesis 3)\n"," The original paper is entirely based on 1-D CNNs and the raw ECG signal. To extend the paper's results, we aim to pre-process the raw signals using Fourier transforms to represent the data as a spectogram -- a frequency versus time representation of ECG signals. Using this representation of the input, we will train a 2-D CNN model (i.e. 2-D ResNetv18) and compare the performance of pre-trained and randomly initialized models. Additionally, we will compare the 2-D model performance to the 1-D models originally used by the authors.\n","\n"," This extension is motivated by a study on ECG Arrhythmia classification that demonstrates the effectiveness of CNNs trained on spectrograms.[6] By converting ECG data to spectrogram features and then using spectrograms to pre-train a 2-D ResNet, we intend to illustrate the adaptability of the transfer learning framework in the original paper across diverse model architectures.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xWAHJ_1CdtaA"},"source":["# Methodology\n","\n","## Environment\n","First, we assume that this notebook is run in **Google Colab**.\n","\n","All of the project's codes, data, and files are in a shared Google drive. Users of this notebook are required to mount the shared drive in a Google Colab notebook.\n","\n","1. Create a shortcut to the shared Google drive from your own Google Drive.\n","2. Modify the `PROJECT_ROOT` variable below accordingly.\n","\n","The link to the project's shared Google drive: https://drive.google.com/drive/folders/1vlUILM7cToH5CoX1x0kWRpe55MbBogS-?usp=drive_link"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jk2v_aWOfdRv"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ONI_oHIHu9Zz"},"outputs":[],"source":["# modify based on shortcut to shared Google drive\n","PROJECT_ROOT = '/content/drive/MyDrive/UIUC MCS/CS598 Deep Learning for Healthcare/Project'\n","DATA_DIR = PROJECT_ROOT + '/data'"]},{"cell_type":"markdown","metadata":{"id":"8_d-BnwhwM60"},"source":["Install required packages. Note that the `%%capture` cell magic is used to suppress the output of `pip install` for brevity."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xZWw8dgEtREL"},"outputs":[],"source":["%%capture\n","%cd $PROJECT_ROOT\n","! pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"2NbPHUTMbkD3"},"source":["##  Data\n","\n","### Pre-training Dataset\n","The training data is the \"Icentia11k Single Lead Continuous Raw Electrocardiogram Dataset,\" which is freely available online.\n","* Source of the data\n","  * https://physionet.org/content/icentia11k-continuous-ecg/1.0/ (raw)\n","  * https://academictorrents.com/details/af04abfe9a3c96b30e5dd029eb185e19a7055272 (compressed)\n","* Statistics\n","  * 11,000 patients\n","  * Each patient has up to two weeks of ECG recordings with 250 Hz sampling rate.\n","  * Each ECG recording is accompanied with beat and rhythm labels marked by the ECG signal collection device and specialists, respectively.\n","  * Both beat and rhythm labels are assigned to positions in the signal at irregular intervals.\n","  * The original paper uses 95% of the patients for pre-training and the remaining 5% for validation.\n","* Data downloading:\n","   * We utilize the compressed data files rather than the raw files.\n","   * The process is demonstrated in [this notebook](https://github.com/myles-i/DLH_TransferLearning/blob/master/jupyter_notebooks/Download_Icentia11k_Data.ipynb), which uses the `libtorrent` library to download the compressed version of the data from academictorrents.com.\n","   * The compressed data files are saved to an appropriately named sub-directory within the shared Google Drive directory `DATA_DIR`.\n","\n","\n","### Fine-tuning Dataset\n","The fine-tuning dataset is the \"AF Classification from a Short Single Lead ECG Recording: The PhysioNet/Computing in Cardiology Challenge 2017\" and freely avaiable online for download.\n","* Source of the data\n","  * https://physionet.org/content/challenge-2017/1.0.0/\n","* Statistics\n","  * 8528 short ECG recordings\n","  * Each ECG recording duration is 9 to 60 seconds with 300 Hz sampling rate\n","  * Each ECG recording is labeled with one of the following classes: AF, Normal, Other or Noise (too noisy to classify).\n","  \n","* Data process (TODO: add more detail)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3muyDPFPbozY"},"source":["## Model used by authors\n","In this project, CNN model of choice is ResNet-18v2. 1-D ResNet-18v2 implemented in the paper's github [2] is used to reproduce the paper's result, and 2D ResNet-18 for abalation task.\n","\n","TODO: note difference with fine-tuning resnet\n","\n","### 1-D ResNet-18v2\n","* Model architecture\n","  * 18 layers\n","  * Input layer consists of convolution layer with 64 filters, kernel size=3 and stride=2. The output of the convolution layer passes through batch norm, ReLu and maxpooling layers sequentially.\n","  * Output layer is a classifier consisting of a densely-connected layer followed by softmax function.\n","  * The middle 16 layers consist of 8 residual blocks. A residual block consists of the following two components and outputs the sum of the two components' outputs.\n","    1. Two convolution layers, each followed by batch norm and ReLu.\n","    2. A shortcut that passes the input through a convolution layer followed by batch norm.\n","  * Configurations of the residual blocks\n","    * 1st and 2nd: 64 filters, kernel size=7, strides=2 and 1, respectively\n","    * 3rd and 4th: 128 filters, kernel size=5, strides=2 and 1, respectively\n","    * 5th and 6th: 256 filters, kernel size=5, strides=2 and 1, respectively\n","    * 7th and 8th: 512 filters, kernel size=3, strides=2 and 1, respectively\n","  * Detail: https://github.com/myles-i/DLH_TransferLearning/blob/master/transplant/modules/resnet1d.py\n","\n","* Training objectives\n","  * Loss function: `tf.keras.optimizers.Adam(beta_1=0.9, beta_2=0.98, epsilon=1e-9)`\n","  * Optimizer: `tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)`\n","  * Metric: `tf.keras.metrics.SparseCategoricalAccuracy(name='acc')`\n"]},{"cell_type":"markdown","metadata":{"id":"PjhVgufs6OIR"},"source":["## Training\n","\n","### Computational Requirements\n","\n","We use Google Colab Pro, V100 GPU (16 GB RAM).\n","\n","### Implementation code\n","\n","For both pre-training and fine-tuning, the paper provides entrypoint scripts to run the entire process with parameters of choice. The following is the high level description of how the scripts work.\n","\n","#### Pre-training\n","1. Create train/validate data generator based on patient id and the number of samples per patient, both specified when calling the API.\n","2. A model is generated based on the model architectrue and pre-training task specified by the API user.\n","3. Weights of the model are initialized. They can also be loaded from a weights file. For all pre-training in the project, we don't load weights.\n","4. Check point function is created based on training metric. For pre-training, we use `loss` as training metric.\n","5. The model fits the train data. At the end of each training epoch, the check point function is called for evaluation and save the model weights.\n","\n","#### Fine-tuning\n","1. Train and test data set are already separated from PhysioNet 2017 dataset, with a 80%/20% split, and are passed to the API. The validation dataset will be further sperated from the train dataset based on user input.\n","2. A CNN model is generated based on the model architectrue specified by the API user. A binary classifier is attached to the output of the CNN model as output layer.\n","3. Weights of the model are initialized. They can also be loaded from a weights file.\n","4. Check point function is created based on training metric. For pre-training, we use `f1` as training metric.\n","5. The model fits the train data. At the end of each training epoch, the check point function is called for evaluation and save the model weights.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"40VeOBV16OIR"},"source":["## Evaluation\n","\n","### Metrics Descriptions\n","\n","In fine-tuning, the paper uses F1 score for evaluating the model on the Physionet validation and test sets.\n","\n","### Implementation code\n","\n","#### Pre-training\n","The paper uses 95% of the patient's ECG data. On average, the paper sample 4096 ECG frames per patient, which amounts to 42.8 million (11000x0.95x4096) training samples over the course of pre-training. For pre-training with 20% of the data used in paper, we use ECG data from 2048 patients and sample 4096 ECG frames per patient, resulting to roughl 8.4 million (2048x4096) training samples."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W9TLC4hj2M72"},"outputs":[],"source":["TRAIN_DATASET = DATA_DIR + '/icentia11k'"]},{"cell_type":"markdown","metadata":{"id":"wnsQfTsc26yA"},"source":["Following uses the API to run pre-training with 20% of the data used in the paper. Uncommet the cell to run.\n","\n","* `--job-dir`: output directory, where check points and weights are saved\n","\n","* `--task`: pre-training task, \"beat\" for Beat classification\n","\n","* `--train`: training dataset directory\n","\n","* `--arch`: CNN architecture\n","\n","* `--patient-ids`: patient id whose ECG data to be used in pre-training\n","\n","* `--frame-size`: number of ECG samples, with 250Hz sampling rate, in a ECG frame\n","\n","* To use all data: number of patients x samples_per_patient = epochs x batch_size x steps-per-epoch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBVu9Fas18M4"},"outputs":[],"source":["# !time python -m pretraining.trainer \\\n","# --job-dir \"jobs/beat_classification_16epochs_to_20percent_round3\" \\\n","# --task \"beat\" \\\n","# --train $TRAIN_DATASET \\\n","# --arch \"resnet18\" \\\n","# --epochs  16\\\n","# --patient-ids `seq 0 2047 | paste -sd, -` \\\n","# --steps-per-epoch 1024 \\\n","# --samples-per-patient 4096 \\\n","# --batch-size 512 \\\n","# --frame-size 4096"]},{"cell_type":"markdown","metadata":{"id":"tAzi8Aa06Zh2"},"source":["#### Fine-tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d2TJ3mzh6wUB"},"outputs":[],"source":["JOB_DIR = PROJECT_ROOT + '/jobs/fine_tune_random_resnet18'\n","FINETUNE_TRAIN = DATA_DIR + '/physionet_finetune/physionet_train.pkl'\n","FINETUNE_TEST = DATA_DIR + '/physionet_finetune/physionet_test.pkl'"]},{"cell_type":"markdown","metadata":{"id":"Yd-4RslV7b0o"},"source":["The following uses the fine-tuning entrypoint to run fine-tuning with random initialization. Uncomment to run.\n","* `--weights-file $WEIGHTS_FILE`: Path to pre-trained weights or a checkpoint of the model to be used for model initialization. Random initialization if not specified.\n","\n","* `--val-size 0.0625`: This is the percentage of the train set size to set aside for the validation set. Note that the PhysioNet data was already split into 80% train, 20% test. The paper uses 5% of the full dataset for validation. We get this via 0.0625x0.8=0.05\n","\n","* `--val-metric \"f1\"`: Use macro F1 score to evaluate performance on validation set and to find the best model at each epoch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sV1EBd7Z6nr_"},"outputs":[],"source":["# %%time\n","# !python -m finetuning.trainer \\\n","# --job-dir $JOB_DIR \\\n","# --train $FINETUNE_TRAIN \\\n","# --test $FINETUNE_TEST \\\n","# --val-size 0.0625 \\\n","# --val-metric \"f1\" \\\n","# --arch \"resnet18\" \\\n","# --batch-size 128 \\\n","# --epochs 200 \\\n","# --seed 2024 \\\n","# --verbose"]},{"cell_type":"markdown","metadata":{"id":"yNnSDq6w7mDX"},"source":["The following uses the fine-tuning entrypoint to run fine-tuning with pre-training weights. Uncomment to run."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vb51wGvO8Cac"},"outputs":[],"source":["JOB_DIR = PROJECT_ROOT + '/jobs/fine_tune_pre_trained_20_resnet18'\n","WEIGHTS_FILE = PROJECT_ROOT + '/jobs/beat_classification/pre_trained_20_resnet18.weights'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wVpmhIw_7tpO"},"outputs":[],"source":["# %%time\n","# !python -m finetuning.trainer \\\n","# --job-dir $JOB_DIR \\\n","# --train $FINETUNE_TRAIN \\\n","# --test $FINETUNE_TEST \\\n","# --weights-file $WEIGHTS_FILE \\\n","# --val-size 0.0625 \\\n","# --val-metric \"f1\" \\\n","# --arch \"resnet18\" \\\n","# --batch-size 128 \\\n","# --epochs 200 \\\n","# --seed 2024 \\\n","# --verbose"]},{"cell_type":"markdown","metadata":{"id":"gX6bCcZNuxmz"},"source":["# Reproduction Results\n","\n","## Validation F1\n","We compare the validation F1 on the Physionet dataset between the model with random initialization weights and the model with pre-training weights."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LjW9bCkouv8O"},"outputs":[],"source":["import pandas as pd\n","from matplotlib import pyplot as plt\n","\n","\n","random_result = pd.read_csv(PROJECT_ROOT + '/jobs/finetune_baseline_65sec/history.csv')\n","pretrain_20_result = pd.read_csv(PROJECT_ROOT + '/jobs/finetune_pretrain_20_weights_65sec/history.csv')\n","\n","plt.plot(random_result['epoch'], random_result['f1'], color='tab:red', label='random')\n","plt.plot(pretrain_20_result['epoch'], pretrain_20_result['f1'], color='blue', label='pretrain 20')\n","plt.xlabel('Epoch')\n","plt.ylabel('Validation F1')\n","ax = plt.gca()\n","ax.set_xlim([0, 80])\n","ax.set_ylim([0.5, 1.0])\n","plt.legend(loc='upper right')"]},{"cell_type":"markdown","metadata":{"id":"o-7ii-4FA6Og"},"source":["### Reproduction Analysis\n","With pre-training weights, the F1 scores stablize much sooner than random initialization. The model with pre-training weights also outperforms the random initalization model for the most part."]},{"cell_type":"markdown","metadata":{},"source":["## Test F1 TODO\n","\n","TODO: run our my_f1 function on the test.csv files for both types of models"]},{"cell_type":"markdown","metadata":{},"source":["## Planned next steps\n","The paper runs fine-tuning 10 times for each weights initialization method and then plots the F1 versus Epoch graph. So far, we have only run fine-tuning once for each of randomly initialized network and pre-trained network. We can run fine-tuning more times and then compare with Figure 3(a) in the paper again."]},{"cell_type":"markdown","metadata":{"id":"cxA8ecxzthX5"},"source":["# Ablation Study (Hypothesis 3)\n","For the ablation study, we are pre-processing the data to produce a spectogram for each ECG signal and then passing this result into a 2-D ResNet-18v2. Then, the same pre-training and fine-tuning process from the paper is followed. The goal is to test how extensible the papers ideas are to other models. The work-in-progress notebook that includes building the model in `keras`, the data pre-processing function, the `keras` data generators, and running two epochs of pre-training. \n","\n","Fine-tuning is not yet complete:\n","\n","https://github.com/myles-i/DLH_TransferLearning/blob/master/jupyter_notebooks/explore_spectogram.ipynb\n","\n","Below, we explain the preprocessing and model in more detail:\n","\n","### Ablation Preprocessing - spectogram\n","For the spectogram, the following parameters were chosen:\n","- window_size: 256 (~1 second)\n","- stride: 64 (~0.25 seconds)\n","- window type: hanning -> this is like a pre-defined convolution that isused to smooth the FFTs for each spectogram slice\n","\n","After the spectogram for a data sample is computed, only the lower 32 frequency components (from 0-62.5 Hz) are selected to reduce the input size. This was chosen empirically and validated by reproducing the original signal with the filtered spectogram models.\n","\n","Below shows an example input sample, and its spectogram:\n","\n","![sample_signal](https://github.com/myles-i/DLH_TransferLearning/blob/master/report/images/sample_signal.PNG?raw=1)\n","\n","![sample_spectogram](https://github.com/myles-i/DLH_TransferLearning/blob/master/report/images/sample_spectogram.PNG?raw=1)\n","\n","\n","### Ablation Model: 2-D ResNet-18v2\n","The model chose for the ablation study using spectograms is similar to the original model used, but is a 2-D ResNet-18v2. It is presented here:\n","* Model architecture\n","  * 18 layers\n","  * Input layer consists of convolution layer with 64 filters, kernel size=7x7 and stride=2. The output of the convolution layer passes through batch norm, ReLu and maxpooling layer sequentially.\n","  * The middle 16 layers consists of 8 residual blocks. A residual block consists of the following two components and outputs the sum of the two components' outputs.\n","    1. Two convolution layers, each followed by batch norm and ReLu\n","    2. A shortcut that passes the input through a convolution layer followed by batch norm.\n","  * Output layer is a classifier consisting of a densely-connected layer followed by softmax or sigmoid function.\n","  * Configurations of the residual blocks\n","    * 1st and 2nd: 64 filters, kernel size=3x3, strides=2 and 1, respectively.\n","    * 3rd and 4th: 128 filters, kernel size=3x3, strides=2 and 1, respectively.\n","    * 5th and 6th: 256 filters, kernel size=3x3, strides=2 and 1, respectively.\n","    * 7th and 8th: 512 filters, kernel size=3x3, strides=2 and 1, respectively.\n","\n","* Training objectives\n","  * loss function: `tf.keras.optimizers.Adam(beta_1=0.9, beta_2=0.98, epsilon=1e-9)`\n","  * optimizer: `tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)`\n","  * metric: `tf.keras.metrics.SparseCategoricalAccuracy(name='acc')`\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# References\n","\n","1. Weimann, K., Conrad, T.O.F. Transfer learning for ECG classification. Sci Rep 11, 5251 (2021). https://doi.org/10.1038/s41598-021-84374-8\n","2. https://github.com/kweimann/ecg-transfer-learning\n","3. Kachuee,M.,Fazeli,S.,&Sarrafzadeh,M.ECGheartbeatclassification:adeeptransferablerepresentation.in2018IEEEInterna- tional Conference on Healthcare Informatics (ICHI)https://doi.org/10.1109/ichi.2018.00092 (2018).\n","4. Rahhal, M. A. et al. Deep learning approach for active classification of electrocardiogram signals. Inf. Sci. 345, 340–354. https:// doi.org/10.1016/j.ins.2016.01.082 (2016).\n","5. Rajan, D., Beymer, D., & Narayan, G. Generalization Studies of Neural Network Models for Cardiac Disease Detection Using Limited Channel ECG (2019). arXiv:1901.03295.\n","6. J. Huang, B. Chen, B. Yao and W. He, “ECG Arrhythmia Classification Using STFT-Based Spectrogram and Convolutional\n"," Neural Network,” in IEEE Access, vol. 7\n","\n"]}],"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"10CnAxawFisX2w0kj3byNjp5CEMio8aqr","timestamp":1712988627392},{"file_id":"1MGxB_J2TvhAANcQG8VNMvQp1QdQrcxWb","timestamp":1709325387341},{"file_id":"1oAKqszNlwEZwPa_BjHPqfcoWlikYBpi5","timestamp":1709153069464}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
