{"cells":[{"cell_type":"markdown","metadata":{"id":"j01aH0PR4Sg-"},"source":["# FAQ and Attentions (TODO Remove)\n","* any report must have run-able codes and necessary annotations (in text and code comments).\n","* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n","must be within **8 min**, otherwise, you may get penalty on the grade.\n","  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n","  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n","  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n","* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n","* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n","* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary."]},{"cell_type":"markdown","metadata":{"id":"dlv6knX04FiY"},"source":["# Mount Notebook to Google Drive (TODO remove this markdown)\n","Upload the data, pretrianed model, figures, etc to your Google Drive, then mount this notebook to Google Drive. After that, you can access the resources freely.\n"]},{"cell_type":"markdown","metadata":{"id":"ypFTXiFx6OIM"},"source":["# Basic Info\n","\n","## Team 1\n","- Ted Hsu (thhsu4@illinois.edu)\n","- Myles Iribarne (mylesai2@illinois.edu)\n","- Daniel Xu (dhxu2@illinois.edu)\n","\n","## Paper\n","\n","Our paper is _Transfer learning for ECG classification_ by Weimann and Conrad [1]. The project code is available on Github [2].\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MQ0sNuMePBXx"},"source":["# Introduction\n","## Background of the problem\n","\n","- **What type of problem**:\n","\n","  The problem is to classify Atrial Fibrillation (AF) on electrocardiogram (ECG) recordings.\n","- **What is the importance/meaning of solving the problem**:\n","\n","  - A solution to the problem is a tool that will assist physicians in analyzing large amounts of patient ECG data in an automated and time efficient manner.\n","  - Early detection of AF events may lead to better patient outcomes.\n","  \n","- **What is the difficulty of the problem**:\n","\n","  - Devices for recording patient ECG data are able to output a _huge_ amount of raw data. This is challenging and expensive to annotate for effective Deep Learning training.\n","  - Large class imbalance due to cardiovascular events of interests being rare.\n","  - Low ECG signal quality due to sampling frequency, single ECG lead probe.\n","\n","- **The state of the art methods and effectiveness**:\n","\n","  - Transfer learning using 1-D residual networks [3]\n","  - Representation learning using encoder-decoder architectures\n","    - Stacked Denoising AEs [4]\n","    - Seq2Seq model [5]\n","\n","## Paper Explanation\n","- **What did the paper propose**:\n","\n","  - Use Transfer learning to build better ECG classifiers.\n","  - Pre-train 1-D CNNs on the largest publicly available ECG dataset (_Icentia11K_) on several pre-training tasks:\n","    - Beat Classification\n","    - Rhythm Classifcation\n","    - Heart Rate Classification\n","    - Future Prediction\n","  - Finetune the pre-trained 1-D CNNs on a _different_ task and a _differrent_ dataset (_PhysioNet/CinC Challenge 2017_): classify AF events.\n","\n","- **What is/are the innovations of the method**:\n","\n","  - Demonstration of successful large-scale pre-training of 1-D CNNs on the largest publicly available ECG dataset to date.\n","  - Demonstration of contrastive pre-training (unsupervised representation learning) improving 1-D CNN performance on target task.\n","  - Novel usage of heart rate classifiction task for pre-training. Note that in this task, the labels can be automatically generated without manual intervention.\n","\n","- **How well the proposed method work (in its own metrics)**:\n","\n","  - The paper provides AF classifier performnace comparison among five different pre-training tasks configurations (random initalization, Beat classification, Phythm classification, Heart Rate classification, and Future Prediciton). Average macro F1 score of the AF classifier on test set is the prformance metric.\n","  - The macro F1 score of random initalization pre-training task is 0.731. F1 scores reported by all proposed four pre-training tasks configurations range from 0.742 to 0.779.\n","\n","- **What is the contribution to the research regime (referring the Background above, how important the paper is to the problem)**:\n","\n","  - Pre-training the 1-D CNN model improves the perofrmance on the target task (i.e. AF classification), effectively reducing the number of labeled data required to achieve the same performance as 1-D CNNs that are not pre-trained.\n","  - Unsupervised pre-training (i.e. future prediction) on ECG data is a viable method for improving the performance on the target task and will become more relevant, since labeling ECG data is expensive."]},{"cell_type":"markdown","metadata":{"id":"uygL9tTPSVHB"},"source":["# Scope of Reproducibility\n","\n","##Hypothesis 1\n","Pre-training 1-D CNN models with an extremely large dataset of relatively inexpensively labeled data can improve performance of classification based on a smaller set of labeled data with a different classification objective (i.e. AF).\n","\n","##Hypothesis 2\n"," The paper does not explore how significant the effects of the pre-training data size are on the final results. We assume size of the pre-training dataset could affect the performance of the target task (i.e. AF classification).\n","\n","\n","##Verification\n","We will verify the hypothesises by attempting to reproduce results for a specific model and the folloing hyperparameter combination with 10% and 20% of the pre-training data:\n","\n","- Model: 1-D ResNet-18v2\n","- Pre-training Objective: Beat Classification\n","- Frame Size: 4096 (samples)\n","- Sample Rate: 250 Hz\n","- Fine-tuning objective: Atrial Fibrillation\n","\n","The results will be compared with the performance of random initilization.\n","\n"]},{"cell_type":"markdown","source":["# Ablation (TODO: remove from draft)\n"," The original paper is entirely based on 1-D CNNs and the raw ECG signal. To extend the paper's results, we aim to pre-process the raw signals using fourier transforms to represent the data as a spectogram  - a frequency versus time representation of ECG signals. Using this representation of the input, we will train a 2-D CNN model (i.e. 2-D ResNet) and compare performance of pre-trained and randomly initialized models, as well as compare the performance to the 1-D models originally used by the authors.\n","\n"," This extension is motivated by a study on ECG Arrhythmia classification that demonstrates the effectiveness of CNNs trained on spectrograms.[6] By converting ECG data to spectrograms features and then using spectrograms to pre-train a 2-D ResNet, we intend to illustrate the adaptability of the transfer-learning framework in the original paper across diverse model architectures."],"metadata":{"id":"wcv5QwdvX07h"}},{"cell_type":"markdown","metadata":{"id":"xWAHJ_1CdtaA"},"source":["# Methodology\n","\n","This cell will git clone the repository (https://github.com/myles-i/DLH_TransferLearning), and download a subset of the relevant data to be able to run the rest of the notebook"]},{"cell_type":"code","source":["# !pip install gdown\n","import gdown\n","import os\n","REPO = '/tmp/ecg_transfer_learning'\n","DATA_DIR = '/tmp/ecg_transfer_learning/data'\n","PRETRAINIG_DATA_DIR =  DATA_DIR + \"/icentia11k_mini_subset\"\n","FINETUNIING_DATA_DIR = DATA_DIR + \"/physionet_finetuning\"\n","\n","\n","# clone git repo\n","!git clone https://github.com/myles-i/DLH_TransferLearning.git \"{REPO}\"\n","%cd \"{REPO}\"\n","\n","# download subset of pretraining data\n","url = \"https://drive.google.com/drive/folders/1pz8V78Qog9nQzfDCjHOrifkDQWg_drIK?usp=sharing\"\n","os.makedirs(PRETRAINIG_DATA_DIR, exist_ok=True)\n","gdown.download_folder(url, output = PRETRAINIG_DATA_DIR)\n","\n","# download finetuning data\n","url = \"https://drive.google.com/drive/folders/10D7orBB6SWB3JRUK9GFw8w_QzDu509D7?usp=sharing\"\n","os.makedirs(FINETUNIING_DATA_DIR, exist_ok=True)\n","gdown.download_folder(url, output = FINETUNIING_DATA_DIR)"],"metadata":{"id":"ONI_oHIHu9Zz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["install required packages"],"metadata":{"id":"8_d-BnwhwM60"}},{"cell_type":"code","source":["! pip install -r requirements.txt"],"metadata":{"id":"xZWw8dgEtREL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2NbPHUTMbkD3"},"source":["##  Data\n","\n","### Pre-training Dataset\n"," The training data is the “Icentia11k Single Lead Continuous Raw Electrocardiogram Dataset,” which is freely available online.\n"," * Source of the data\n","  * https://physionet.org/content/icentia11k-continuous-ecg/1.0/ (raw)\n","  * https://academictorrents.com/details/af04abfe9a3c96b30e5dd029eb185e19a7055272 (271GB compressed)\n"," * Statistics\n","  * 11,000 patients\n","  * Each patient has upto two weeks of ECG recordings with 250Hz sampling rate.\n","  * Each ECG recording is acompanied with beat and rhythm labels marked by the ECG signal collection device and specialists, respectively.\n","  * Both beat and rhythm labels are assigned to positions in the signal at irregular intervals.\n","  * The original paper uses 95% of the patients for pre-training and the remaining 5% for validation.\n"," * Data downlaoding:\n","   * See [this notebook](https://github.com/myles-i/DLH_TransferLearning/blob/master/jupyter_notebooks/Download_Icentia11k_Data.ipynb) which uses the python libtorrent library to download the compressed version of the data from academictorrents.com\n","   \n","\n","\n","\n","\n","### Fine-tuning Dataset\n","The fine-tuning dataset is the “AF Classification from a Short Single Lead ECG Recording: The PhysioNet/Computing in Cardiology Challenge 2017” and freely avaiable online for download.\n"," * Source of the data\n","  * https://physionet.org/content/challenge-2017/1.0.0/\n"," * Statistics\n","  * 8528 short ECG recordings\n","  * Each ECG recording duration is 9 to 60 seconds with 300Hz sampling rate\n","  * Each ECG recording is labeled with one of the following classes: AF, Normal, Other or Noise (too noisy to classify).\n"," * Data process\n","\n","### SP 24 rubrik final report\n","- Data download instruction\n","- Data descriptions with helpful charts and visualizations\n","- Preprocessing code + command\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BZScZNbROw-N"},"outputs":[],"source":["from transplant.datasets.icentia11k import load_patient_data\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# lets load one patients data and plot an ecg signal\n","patient_id = 1\n","data_idx = 1\n","(ecg_signal, labels) = load_patient_data(PRETRAINIG_DATA_DIR, patient_id, include_labels=True, unzipped=False)\n","\n","# lets explore the data a bit\n","print(\"Patient id: \", patient_id)\n","print(\"Number of ECG signals: \", len(ecg_signal))\n","print(\"Length of of ECG at index: \", len(ecg_signal[data_idx]))\n","\n","# now lets plot the data\n","fsample = 250.0;\n","T_x = 1.0/fsample\n","N = 500\n","t_x = np.arange(N) * T_x\n","selected_signal =ecg_signal[data_idx][:N];\n","plt.plot(t_x, selected_signal)\n","title = f\"ECG signal for patient {patient_id} data index {data_idx} of length {len(selected_signal)} \"\n","plt.title(title)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"3muyDPFPbozY"},"source":["##   Model\n","In this porject, CNN model of the choice is ResNet-18. 1D ResNet-18 implemented in the paper's github [2] is used to reproduce the paper's result, and 2D ResNet-18 for abalation task.\n","\n","### 1D ResNet-18\n","  * Model architecture\n","    * 18 layers\n","    * Input layer consists of convolution layer with 64 filters, kernel size=3 and stride=2. The output of the convolution layer passes through batch norm, ReLu and maxpooling layer sequentially.\n","    * Output layer is a clssifier consisting of densely-connected layer followed by softmax function.\n","    * The middle 16 layers consists of 8 residual blocks. A residual block consists of the following two components and outputs the sum of the two compoenets' outputs.\n","      1. two convolution layers, each followed by batch norm and ReLu\n","      2. a shortcut that passes the input through a convolution layer followed by batch norm.\n","    * Configurations of the residual blocks\n","      * 1st and 2nd: 64 filters, kernel size=7, strides=2 and 1, respectively\n","      * 3rd and 4th: 128 filters, kernel size=5, strides=2 and 1, respectively\n","      * 5th and 6th: 256 filters, kernel size=5, strides=2 and 1, respectively\n","      * 7th and 8th: 512 filters, kernel size=3, strides=2 and 1, respectively\n","    * Detail: https://github.com/myles-i/DLH_TransferLearning/blob/master/transplant/modules/resnet1d.py\n","\n","  * Training objectives\n","    * loss function: `tf.keras.optimizers.Adam(beta_1=0.9, beta_2=0.98, epsilon=1e-9)`\n","    * optimizer: `tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)`\n","    * metric: `tf.keras.metrics.SparseCategoricalAccuracy(name='acc')`\n","\n","### 2D ResNet-18 (TODO in final report)\n","\n","\n","### SP24 Rubrik final report\n","\n","- Citation to the original paper\n","- Link to the original paper’s repo (if applicable)\n","- Model descriptions\n","- Implementation code\n","- Pretrained model (if applicable)\n"]},{"cell_type":"markdown","metadata":{"id":"PjhVgufs6OIR"},"source":["## Training\n","\n","### Computational Requirements\n","\n","We use Google Colab Pro, V100 GPUs.\n","\n","### Implementation code\n","\n","For both pre-training and fine tuning, the paper provides API to run the entire process with parameters of choice. Following is the high level description of how the API works.\n","\n","#### Pre-training\n","  1. Create train/validate data generator based on patient id and the number of samples per patient, both specified when calling the API.\n","  2. A model is generated based on the model architectrue and pre-training task specified by the API user.\n","  3. Weights of the model are initialized. They can also be loaded from a weights file. For all pre-training in the project, we don't load weights.\n","  4. Check point function is created based on training metric. For pre-training, we use `loss` as training metric.\n","  5. The model fits the train data. At the end of each training epoch, the check point function is called for evaluation and save the model weights.\n","\n","#### Fine tuning\n","  1. Train and test data set are already separated from PhysioNet 2017 dataset, with a 80%/20% split, and are passed to the API. The validation dataset will be further sperated from the train dataset based on user input.\n","  2. A CNN model is generated based on the model architectrue specified by the API user. A binary classifier is attached to the output of the CNN model as output layer.\n","  3. Weights of the model are initialized. They can also be loaded from a weights file.\n","  4. Check point function is created based on training metric. For pre-training, we use `f1` as training metric.\n","  5. The model fits the train data. At the end of each training epoch, the check point function is called for evaluation and save the model weights.\n","\n","### SP24 Final report rubrik\n","\n","Hyperparams\n","- Report at least 3 types of hyperparameters such as learning rate, batch size, hidden size, dropout\n","\n","Computational requirements\n","- Report at least 3 types of requirements such as type of hardware, avg runtime for each epoch, total number of trial, GPU hrs used, # training epochs\n","- Training code\n"]},{"cell_type":"markdown","metadata":{"id":"40VeOBV16OIR"},"source":["## Evaluation\n","\n","### Metrics Descriptions\n","\n","F1 metric\n","\n","### Implementation code\n","\n","####Pre-training\n","The paper uses 95% of the patient's ECG data. On average, the paper sample 4096 ECG frames per patient, which amounts to 42.8 million (11000x0.95x4096) training samples over the course of pretraining. For pre-training with 20% of the data used in paper, we use ECG data from 2048 patients and sample 4096 ECG frames per patient, resulting to roughl 8.4 million (2048x4096) training samples.\n"]},{"cell_type":"code","source":["TRAIN_DATASET = DATA_DIR + '/icentia11k'"],"metadata":{"id":"w3cIloB1HXeI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Following uses the API to run pre-training with 20% of the data used in the paper. Uncommet the cell to run.\n","\n","* `--job-dir`: output directory, where check points and weights are saved\n","\n","* `--task`: pre-training task, \"beat\" for Beat classification\n","\n","* `--train`: training dataset directory\n","\n","* `--arch`: CNN architecture\n","\n","* `--patient-ids`: patient id whose ECG data to be used in pre-training\n","\n","* `--frame-size`: number of ECG samples, with 250Hz sampling rate, in a ECG frame\n","\n","* To use all data: number of patients x samples_per_patient = epochs x batch_size x steps-per-epoch."],"metadata":{"id":"NKX3Om3hHbuY"}},{"cell_type":"code","source":["# !time python -m pretraining.trainer \\\n","# --job-dir \"jobs/beat_classification_16epochs_to_20percent_round3\" \\\n","# --task \"beat\" \\\n","# --train $TRAIN_DATASET \\\n","# --arch \"resnet18\" \\\n","# --epochs  16\\\n","# --patient-ids `seq 0 2047 | paste -sd, -` \\\n","# --steps-per-epoch 1024 \\\n","# --samples-per-patient 4096 \\\n","# --batch-size 512 \\\n","# --frame-size 4096"],"metadata":{"id":"G-qkPo6RHchx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Fine Tuning"],"metadata":{"id":"s8cXqzeoHgnB"}},{"cell_type":"code","source":["JOB_DIR = PROJECT_ROOT + '/jobs/fine_tune_random_resnet18'\n","FINETUNE_TRAIN = DATA_DIR + '/physionet_finetune/physionet_train.pkl'\n","FINETUNE_TEST = DATA_DIR + '/physionet_finetune/physionet_test.pkl'"],"metadata":{"id":"J9XIXUrSHf2R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Following uses the API to run fine tuning with random initialization. Uncomment to run.\n","* `--weights-file $WEIGHTS_FILE`: Path to pretrained weights or a checkpoint of the model to be used for model initialization. Random initilization if not specified.\n","\n","* `--val-size 0.0625`: This is the percentage of the train set size to set aside for the validation set. Note that the PhysioNet data was already split 80-20 train-test. The paper uses 5 percent of the full dataset for validation. We get this via 0.0625x0.8=0.05\n","\n","* `--val-metric \"f1\"`: Use macro F1 score to evaluate performance on validation set and to find the best model at each epoch."],"metadata":{"id":"ujdlrNLrHp9-"}},{"cell_type":"code","source":["# %%time\n","# !python -m finetuning.trainer \\\n","# --job-dir $JOB_DIR \\\n","# --train $FINETUNE_TRAIN \\\n","# --test $FINETUNE_TEST \\\n","# --val-size 0.0625 \\\n","# --val-metric \"f1\" \\\n","# --arch \"resnet18\" \\\n","# --batch-size 128 \\\n","# --epochs 200 \\\n","# --seed 2024 \\\n","# --verbose"],"metadata":{"id":"dHoEwoxzHptl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Following uses the API to run fine tuning with pre-traning weights. Uncomment to run."],"metadata":{"id":"ncKW_2t1Hvfv"}},{"cell_type":"code","source":["JOB_DIR = PROJECT_ROOT + '/jobs/fine_tune_pre_trained_20_resnet18'\n","WEIGHTS_FILE = PROJECT_ROOT + '/jobs/beat_classification/pre_trained_20_resnet18.weights'"],"metadata":{"id":"XhPZMj0JHpmQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# %%time\n","# !python -m finetuning.trainer \\\n","# --job-dir $JOB_DIR \\\n","# --train $FINETUNE_TRAIN \\\n","# --test $FINETUNE_TEST \\\n","# --weights-file $WEIGHTS_FILE \\\n","# --val-size 0.0625 \\\n","# --val-metric \"f1\" \\\n","# --arch \"resnet18\" \\\n","# --batch-size 128 \\\n","# --epochs 200 \\\n","# --seed 2024 \\\n","# --verbose"],"metadata":{"id":"Md420mWqH0yO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gX6bCcZNuxmz"},"source":["# Results\n","We compare the validation F1 between model with random initialization weights and pre-training weights.\n","\n","\n","## SP24 final report rubrik\n","\n","- Table of results (no need to include additional experiments, but main reproducibility result should be included)\n","- All claims should be supported by experiment results\n","- Discuss with respect to the hypothesis and results from the original paper\n","- Experiments beyond the original paper\n","    - Credits for each experiment depend on how hard it is to run the experiments. Each experiment should include results and discussion\n","    - Ablation Study.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LjW9bCkouv8O"},"outputs":[],"source":["import pandas as pd\n","from matplotlib import pyplot as plt\n","\n","\n","random_result = pd.read_csv(PROJECT_ROOT + '/jobs/finetune_baseline_65sec/history.csv')\n","pretrain_20_result = pd.read_csv(PROJECT_ROOT + '/jobs/finetune_pretrain_20_weights_65sec/history.csv')\n","\n","plt.plot(random_result['epoch'], random_result['f1'], color='tab:red', label='random')\n","plt.plot(pretrain_20_result['epoch'], pretrain_20_result['f1'], color='blue', label='pretrain 20')\n","plt.xlabel('Epoch')\n","plt.ylabel('Validation F1')\n","ax = plt.gca()\n","ax.set_xlim([0, 80])\n","ax.set_ylim([0.5, 1.0])\n","plt.legend(loc='upper right')"]},{"cell_type":"markdown","source":["## Analysis\n","With pre-training weights, the F1 scores stablize much sooner than random initialization. Model with pre-trainin weights also outperform random initalization model in the most part.\n","\n","## Plan\n","The paper runs fine tuning 10 times for each weights initilization method and then plot the F1 versus Epoch graph. We only run fine tuning once for each  weights initilization method. We can run fine tuning more times and then compare with Figure 3 (a) in the paper again."],"metadata":{"id":"EBGlXIhBIEQb"}},{"cell_type":"markdown","metadata":{"id":"8EAWAy_LwHlV"},"source":["## Model comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uOdhGrbwwG71"},"outputs":[],"source":["# compare you model with others\n","# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"]},{"cell_type":"markdown","metadata":{"id":"qH75TNU71eRH"},"source":["# Discussion\n","\n","In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n","  * Make assessment that the paper is reproducible or not.\n","  * Explain why it is not reproducible if your results are kind negative.\n","  * Describe “What was easy” and “What was difficult” during the reproduction.\n","  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n","  * What will you do in next phase.\n","\n","## SP24 Final report rubrik\n","\n","- Implications of the experimental results, whether the original paper was reproducible, and if it wasn’t, what factors made it irreproducible\n","- “What was easy”\n","- “What was difficult”\n","- Recommendations to the original authors or others who work in this area for improving reproducibility\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E2VDXo5F4Frm"},"outputs":[],"source":["# no code is required for this section\n","'''\n","if you want to use an image outside this notebook for explanaition,\n","you can read and plot it here like the Scope of Reproducibility\n","'''"]},{"cell_type":"markdown","metadata":{"id":"T__pMH5N6OIT"},"source":["# Public GitHub Repo\n","\n","Under construction, not required for draft"]},{"cell_type":"markdown","metadata":{"id":"SHMI2chl9omn"},"source":["# References\n","\n","1. Weimann, K., Conrad, T.O.F. Transfer learning for ECG classification. Sci Rep 11, 5251 (2021). https://doi.org/10.1038/s41598-021-84374-8\n","2. https://github.com/kweimann/ecg-transfer-learning\n","3. Kachuee,M.,Fazeli,S.,&Sarrafzadeh,M.ECGheartbeatclassification:adeeptransferablerepresentation.in2018IEEEInterna- tional Conference on Healthcare Informatics (ICHI)https://doi.org/10.1109/ichi.2018.00092 (2018).\n","4. Rahhal, M. A. et al. Deep learning approach for active classification of electrocardiogram signals. Inf. Sci. 345, 340–354. https:// doi.org/10.1016/j.ins.2016.01.082 (2016).\n","5. Rajan, D., Beymer, D., & Narayan, G. Generalization Studies of Neural Network Models for Cardiac Disease Detection Using Limited Channel ECG (2019). arXiv:1901.03295.\n","6. J. Huang, B. Chen, B. Yao and W. He, “ECG Arrhythmia Classification Using STFT-Based Spectrogram and Convolutional\n"," Neural Network,” in IEEE Access, vol. 7\n","\n"]}],"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1MGxB_J2TvhAANcQG8VNMvQp1QdQrcxWb","timestamp":1709325387341},{"file_id":"1oAKqszNlwEZwPa_BjHPqfcoWlikYBpi5","timestamp":1709153069464}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}